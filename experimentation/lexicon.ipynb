{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **CHILDES Lexicon Builder**\n",
        "\n",
        "This notebook constructs a lexical dictionary from the CHILDES IPA corpus.\n",
        "The lexicon is used to gate phonological rules, ensuring corrective modifications to IPA strings are only applied when the resulting form corresponds to a real English word.\n",
        "\n",
        "This prevents false-positive corrections—particularly important for child-speech IPA where pronunciations often deviate from canonical adult forms.\n",
        "\n",
        "### **1. Environment Setup**\n",
        "\n",
        "The notebook begins by:\n",
        "\n",
        "* Loading required libraries (`pandas`, `json`, `re`, `Counter`)\n",
        "\n",
        "* Setting up file paths to the cleaned CHILDES corpus:\n",
        "```\n",
        "child_train.tsv  \n",
        "child_valid.tsv\n",
        "```\n",
        "\n",
        "* Ensuring a dedicated output directory exists for storing lexicon files.\n",
        "\n",
        "This creates a controlled workspace for lexicon generation."
      ],
      "metadata": {
        "id": "1MYaNsA9gsLp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_MRNfWTz3km",
        "outputId": "24dfff3c-d0d0-416b-dab9-72412babd878"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Mount Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2. Load CHILDES TSV Files (No headers)**\n",
        "\n",
        "Because CHILDES TSVs generated in earlier preprocessing do not include headers, the notebook loads them using:\n",
        "```\n",
        "header=None\n",
        "sep=\"\\t\"\n",
        "```\n",
        "\n",
        "Then renames the two columns:\n",
        "* col0 → ipa_transcription\tcleaned IPA sequence\n",
        "* col1 → gloss\tcleaned text transcript\n",
        "\n",
        "This produces a clean DataFrame containing IPA → Text pairs."
      ],
      "metadata": {
        "id": "3Mnb0CxfhIkm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# LOAD DATASETS WITH NO HEADERS\n",
        "# ============================================================\n",
        "train_path = \"/content/drive/MyDrive/Capstone/Corpus/ipa_childes/child_train.tsv\"\n",
        "valid_path = \"/content/drive/MyDrive/Capstone/Corpus/ipa_childes/child_valid.tsv\"\n",
        "\n",
        "train_df = pd.read_csv(train_path, sep=\"\\t\", header=None)\n",
        "valid_df = pd.read_csv(valid_path, sep=\"\\t\", header=None)\n",
        "\n",
        "df = pd.concat([train_df, valid_df], ignore_index=True)\n",
        "\n",
        "# Based on your file preview:\n",
        "df = df.rename(columns={0: \"ipa_transcription\", 1: \"gloss\"})\n",
        "\n",
        "print(\"Loaded utterances:\", len(df))\n",
        "print(\"Columns:\", df.columns.tolist())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DrEbCSc10EjG",
        "outputId": "3e198128-0c63-47b2-e7f7-aec737ab0d19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded utterances: 184171\n",
            "Columns: ['ipa_transcription', 'gloss']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3. Gloss Preprocessing & Token Extraction**\n",
        "\n",
        "To build a lexicon, the notebook extracts words from gloss strings using:\n",
        "\n",
        "* Lowercasing\n",
        "\n",
        "* Removing punctuation ([^a-z'])\n",
        "\n",
        "* Splitting on whitespace\n",
        "\n",
        "* Removing empty tokens\n",
        "\n",
        "This yields a large list of raw tokens representing the vocabulary used in CHILDES transcripts.\n",
        "\n",
        "These tokens reflect:\n",
        "\n",
        "* child speech\n",
        "\n",
        "* adult interaction speech\n",
        "\n",
        "* frequent, simple English words\n",
        "\n",
        "* morphologically simplified child forms\n",
        "\n",
        "This makes the lexicon appropriate for phonological rule gating."
      ],
      "metadata": {
        "id": "rTpA8WQKhYkb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# CLEAN & TOKENIZE WORDS\n",
        "# ============================================================\n",
        "def clean_word(w):\n",
        "    w = w.lower().strip()\n",
        "    w = re.sub(r\"[^a-z']\", \"\", w)  # keep apostrophes\n",
        "    return w\n",
        "\n",
        "all_words = []\n",
        "\n",
        "for g in df[\"gloss\"].astype(str):\n",
        "    tokens = re.split(r\"\\s+\", g)\n",
        "    for t in tokens:\n",
        "        w = clean_word(t)\n",
        "        if len(w) > 0:\n",
        "            all_words.append(w)\n",
        "\n",
        "print(\"Total raw tokens extracted:\", len(all_words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g9nt9geB0Jsj",
        "outputId": "1ec70a33-a4a0-42b5-c992-850c9b3b373b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total raw tokens extracted: 1334221\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4. Word Frequency Counting**\n",
        "\n",
        "Using `Counter`, the notebook counts total occurrences of each word across both train and validation CHILDES sets.\n",
        "\n",
        "Then it applies a minimum frequency threshold:\n",
        "```\n",
        "MIN_FREQ = 2\n",
        "```\n",
        "\n",
        "This removes rare or noisy words and keeps only those with stable presence in the dataset.\n",
        "\n",
        "The result is a vocabulary of high-confidence English words produced in CHILDES interactions."
      ],
      "metadata": {
        "id": "EHEzcZJJhimw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# COUNT WORD FREQUENCIES\n",
        "# ============================================================\n",
        "freq = Counter(all_words)\n",
        "\n",
        "# Keep words with at least 2 occurrences\n",
        "MIN_FREQ = 2500\n",
        "lexicon_words = [w for w, c in freq.items() if c >= MIN_FREQ]\n",
        "\n",
        "print(\"Words kept in lexicon:\", len(lexicon_words))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Glsw4UZ0bmK",
        "outputId": "fbe8756c-b4af-4306-8491-db4ad9e69bba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Words kept in lexicon: 105\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **5. Final Lexicon Construction**\n",
        "\n",
        "The notebook prepares the final lexicon in three different formats:\n",
        "\n",
        "A. lexicon.txt\n",
        "\n",
        "* One word per line\n",
        "\n",
        "* Easy to inspect manually\n",
        "\n",
        "* Good for debugging lexical matches\n",
        "\n",
        "B. lexicon.json\n",
        "\n",
        "* List of strings\n",
        "\n",
        "* Ideal for programmatic loading\n",
        "\n",
        "* Can be used in other scripts or HF Spaces\n",
        "\n",
        "C. lexicon.py\n",
        "\n",
        "Defines:\n",
        "```\n",
        "LEXICON = {\n",
        "   'cat',\n",
        "   'book',\n",
        "   'car',\n",
        "   ...\n",
        "}\n",
        "```\n",
        "\n",
        "This makes the lexicon importable directly into the rule-based IPA correction script.\n",
        "\n",
        "Example usage:\n",
        "```\n",
        "from lexicon import LEXICON\n",
        "\n",
        "if ipa_to_word(candidate) in LEXICON:\n",
        "    apply_rule()\n",
        "    ```\n",
        "\n",
        "This ensures corrections are linguistically plausible."
      ],
      "metadata": {
        "id": "P4HLLV1EhtVa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# SAVE LEXICON\n",
        "# ============================================================\n",
        "out_dir = \"/content/drive/MyDrive/Capstone/Corpus/lexicon/\"\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "with open(out_dir + \"lexicon.txt\", \"w\") as f:\n",
        "    for w in sorted(lexicon_words):\n",
        "        f.write(w + \"\\n\")\n",
        "\n",
        "with open(out_dir + \"lexicon.json\", \"w\") as f:\n",
        "    json.dump(sorted(lexicon_words), f, indent=2)\n",
        "\n",
        "with open(out_dir + \"lexicon.py\", \"w\") as f:\n",
        "    f.write(\"LEXICON = {\\n\")\n",
        "    for w in sorted(lexicon_words):\n",
        "        f.write(f\"    '{w}',\\n\")\n",
        "    f.write(\"}\\n\")\n",
        "\n",
        "print(\"Lexicon saved to:\", out_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-iW-mEW0di4",
        "outputId": "3a050aae-0ce0-4f22-a565-d5a98bca7df9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lexicon saved to: /content/drive/MyDrive/Capstone/Corpus/lexicon/\n"
          ]
        }
      ]
    }
  ]
}