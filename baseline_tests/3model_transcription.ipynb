{"cells":[{"cell_type":"markdown","metadata":{"id":"29Z-TQkPQnOp"},"source":["# Comparative Baseline: Whisper, Wav2Vec2, and Conformer Models\n","### Zane A. Graper | MSAI 699 Capstone | November 2025\n","\n","This notebook evaluates three pretrained automatic speech recognition (ASR) models on child-speech datasets.\n","\n","**Models Tested:**\n","1. **Whisper-Small (OpenAI)** – a multilingual encoder-decoder transformer trained on 680k hours of labeled data.  \n","2. **Wav2Vec2-Large (Facebook)** – a self-supervised model that learns speech representations from raw audio.  \n","3. **Conformer-CTC (Facebook)** – a convolution-enhanced transformer model optimized for speech recognition.\n","\n","**Goal:**  \n","Measure zero-shot performance of each model on two child-speech datasets:\n","- **TomRoma corpus** (`audiodata/`)  \n","- **CSLU Kids’ Speech corpus** (`kids_speech_wav/`)\n","\n","This establishes a comparative baseline before any domain-specific fine-tuning or phoneme-aware adaptation.\n","\n","### Notebook Setup:"]},{"cell_type":"code","source":["# ==========================================\n","# GOOGLE DRIVE & PATH SETUP\n","# ==========================================\n","from google.colab import drive\n","import os\n","import torch\n","import pandas as pd\n","from tqdm import tqdm\n","\n","drive.mount('/content/drive')\n","\n","BASE_DIR = \"/content/drive/MyDrive/Capstone\"\n","os.environ[\"HF_HOME\"] = f\"{BASE_DIR}/hf_cache\"\n","\n","# Dataset dictionary\n","DATASETS = {\n","    \"tomroma\": {\n","        \"csv\": f\"{BASE_DIR}/Baseline/transcriptions_cleaned.csv\",\n","        \"audio\": f\"{BASE_DIR}/audiodata\"\n","    },\n","    \"cslu\": {\n","        \"csv\": f\"{BASE_DIR}/Baseline/child_speech_cleaned.csv\",\n","        \"audio\": f\"{BASE_DIR}/kids_speech_wav\"\n","    }\n","}\n","\n","device = 0 if torch.cuda.is_available() else -1\n","print(\"Device index for pipelines:\", device)"],"metadata":{"id":"9p7Cn1W0tZcE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Test 1 – Whisper-Small\n","\n","The Whisper-Small model is a 244M-parameter encoder-decoder transformer trained on large multilingual data.  \n","It performs direct end-to-end transcription from waveform to text.\n","\n","This test measures Whisper’s ability to generalize to child voices, which differ acoustically from adult speech.  \n","We expect higher word error rates due to pitch and articulation differences."],"metadata":{"id":"j5sM_9sPuOXW"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"QKnEfeoCzPtA","outputId":"4a3c672d-c0e0-4d29-fd78-96daac73825e","collapsed":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]},{"name":"stderr","output_type":"stream","text":["Device set to use cuda:0\n","100%|██████████| 625/625 [17:09<00:00,  1.65s/it]\n"]}],"source":["# ==========================================\n","# WHISPER-SMALL\n","# ==========================================\n","from transformers import pipeline\n","\n","asr = pipeline(\n","    \"automatic-speech-recognition\",\n","    model=\"openai/whisper-small\",\n","    device=device\n",")\n","\n","for label, paths in DATASETS.items():\n","    csv_path, audio_dir = paths[\"csv\"], paths[\"audio\"]\n","    print(f\"\\n=== Running Whisper-Small on {label.upper()} ===\")\n","\n","    df = pd.read_csv(csv_path)\n","    if \"whisper_small\" not in df.columns:\n","        df[\"whisper_small\"] = \"\"\n","\n","    for i, row in tqdm(df.iterrows(), total=len(df)):\n","        if pd.notna(row[\"whisper_small\"]) and row[\"whisper_small\"].strip():\n","            continue\n","        try:\n","            path = os.path.join(audio_dir, row[\"filename\"])\n","            result = asr(path)\n","            df.at[i, \"whisper_small\"] = result[\"text\"]\n","        except Exception as e:\n","            df.at[i, \"whisper_small\"] = f\"[error: {e}]\"\n","\n","    df.to_csv(csv_path, index=False)\n","    print(f\"✅ Saved results for {label} to {csv_path}\")"]},{"cell_type":"markdown","source":["## Test 2 – Wav2Vec2-Large\n","\n","The Wav2Vec2-Large-960h-lv60 model uses **self-supervised pretraining** on 60k hours of unlabelled speech and fine-tuning on 960h of LibriSpeech.  \n","Unlike Whisper, Wav2Vec2 uses a **CTC (Connectionist Temporal Classification)** head for transcription.\n","\n","This experiment evaluates how well a self-supervised model trained on adult data transcribes children’s speech."],"metadata":{"id":"-TGjhdHvuP67"}},{"cell_type":"markdown","metadata":{"id":"qNR6u94VQsA7"},"source":["## Wav2Vec2"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":142030,"status":"ok","timestamp":1761746943543,"user":{"displayName":"Zane Graper","userId":"12390302578858551947"},"user_tz":300},"id":"CFXC3aKuQurb","outputId":"64b27e61-c1e8-495d-9346-de45c307fe7d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-960h-lv60 and are newly initialized: ['wav2vec2.masked_spec_embed']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Device set to use cuda:0\n","100%|██████████| 625/625 [02:18<00:00,  4.52it/s]\n"]}],"source":["# ==========================================\n","# WAV2VEC2-LARGE\n","# ==========================================\n","from transformers import pipeline\n","\n","asr = pipeline(\n","    \"automatic-speech-recognition\",\n","    model=\"facebook/wav2vec2-large-960h-lv60\",\n","    device=device\n",")\n","\n","for label, paths in DATASETS.items():\n","    csv_path, audio_dir = paths[\"csv\"], paths[\"audio\"]\n","    print(f\"\\n=== Running Wav2Vec2-Large on {label.upper()} ===\")\n","\n","    df = pd.read_csv(csv_path)\n","    if \"wav2vec2\" not in df.columns:\n","        df[\"wav2vec2\"] = \"\"\n","\n","    for i, row in tqdm(df.iterrows(), total=len(df)):\n","        if pd.notna(row[\"wav2vec2\"]) and row[\"wav2vec2\"].strip():\n","            continue\n","        try:\n","            path = os.path.join(audio_dir, row[\"filename\"])\n","            result = asr(path)\n","            df.at[i, \"wav2vec2\"] = result[\"text\"]\n","        except Exception as e:\n","            df.at[i, \"wav2vec2\"] = f\"[error: {e}]\"\n","\n","    df.to_csv(csv_path, index=False)\n","    print(f\"✅ Saved results for {label} to {csv_path}\")"]},{"cell_type":"markdown","metadata":{"id":"k1s2CFm_Q5VX"},"source":["## Test 3 – Conformer-CTC\n","\n","The Conformer model integrates **convolutional modules** into a Transformer backbone to capture both local and global context.  \n","The “rope-large-960h-ft” checkpoint has been fine-tuned for English speech recognition.\n","\n","This test examines whether the architectural changes in Conformer offer better robustness to children’s speech compared to Whisper or Wav2Vec2."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f3B5RsyyRB3d"},"outputs":[],"source":["# ==========================================\n","# CONFORMER-CTC\n","# ==========================================\n","!pip install torchcodec > /dev/null\n","\n","import torchaudio\n","from transformers import Wav2Vec2Processor, Wav2Vec2ConformerForCTC\n","\n","model_name = \"facebook/wav2vec2-conformer-rope-large-960h-ft\"\n","processor = Wav2Vec2Processor.from_pretrained(model_name)\n","model = Wav2Vec2ConformerForCTC.from_pretrained(model_name).to(\"cuda\" if torch.cuda.is_available() else \"cpu\").eval()\n","target_sr = 16000\n","\n","for label, paths in DATASETS.items():\n","    csv_path, audio_dir = paths[\"csv\"], paths[\"audio\"]\n","    print(f\"\\n=== Running Conformer-CTC on {label.upper()} ===\")\n","\n","    df = pd.read_csv(csv_path)\n","    if \"conformer\" not in df.columns:\n","        df[\"conformer\"] = \"\"\n","\n","    for i, row in tqdm(df.iterrows(), total=len(df)):\n","        if pd.notna(row[\"conformer\"]) and row[\"conformer\"].strip():\n","            continue\n","        try:\n","            path = os.path.join(audio_dir, row[\"filename\"])\n","            waveform, sr = torchaudio.load(path, backend=\"ffmpeg\")\n","\n","            # Convert to mono\n","            if waveform.shape[0] > 1:\n","                waveform = torch.mean(waveform, dim=0, keepdim=True)\n","            # Resample if needed\n","            if sr != target_sr:\n","                waveform = torchaudio.transforms.Resample(sr, target_sr)(waveform)\n","\n","            inputs = processor(waveform.squeeze().numpy(), sampling_rate=target_sr, return_tensors=\"pt\", padding=True)\n","            input_values = inputs.input_values.to(model.device)\n","\n","            with torch.inference_mode():\n","                logits = model(input_values).logits\n","                predicted_ids = torch.argmax(logits, dim=-1)\n","                transcription = processor.batch_decode(predicted_ids)[0]\n","\n","            df.at[i, \"conformer\"] = transcription\n","        except Exception as e:\n","            df.at[i, \"conformer\"] = f\"[error: {e}]\"\n","\n","    df.to_csv(csv_path, index=False)\n","    print(f\"✅ Saved results for {label} to {csv_path}\")"]},{"cell_type":"markdown","source":["## Results Summary and Next Steps\n","\n","After processing both the **TomRoma** and **CSLU** datasets with all three models,  \n","the outputs will be evaluated using standard metrics (WER, CER, BLEU).\n","\n","**Expected Observations:**\n","- Whisper may outperform others on longer utterances due to its multilingual pretraining.  \n","- Wav2Vec2 may capture clean acoustic segments but struggle with high-pitch child speech.  \n","- Conformer may yield slightly improved intelligibility due to its convolutional refinement of spectral features.\n","\n","## Evaluation – Compute File-Level Word Error Rates (WER)\n","\n","After transcribing both datasets with all three ASR models, this step evaluates **how accurately each model reproduced the reference transcripts**.\n","\n","For each audio file:\n","- The ground-truth transcript (`transcription`) is compared against the model’s prediction.  \n","- We compute **Word Error Rate (WER)** using the `jiwer` library.  \n","- Punctuation is removed and text is normalized to lowercase for fairness.  \n","- To prevent extreme values from skewing the data, WER values are **capped at 1.0**.\n","\n","The results are saved back into a new CSV for later analysis (`*_wer` columns for each model)."],"metadata":{"id":"tpilFHh5uZu9"}},{"cell_type":"code","source":["# ==========================================\n","# COMPUTE FILE-LEVEL WER FOR ALL MODELS\n","# ==========================================\n","\n","# Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import pandas as pd\n","from jiwer import wer\n","import re, os\n","\n","# ==========================================\n","# CONFIGURATION\n","# ==========================================\n","BASE_DIR = \"/content/drive/MyDrive/Capstone\"\n","FILES = {\n","    \"tomroma\": f\"{BASE_DIR}/Baseline/transcriptions_cleaned.csv\",\n","    \"cslu\": f\"{BASE_DIR}/Baseline/child_speech_cleaned.csv\"\n","}\n","\n","def normalize(s):\n","    \"\"\"Lowercase and remove punctuation/spaces for fair WER comparison.\"\"\"\n","    s = re.sub(r\"[^\\w\\s]\", \"\", str(s).lower())\n","    return re.sub(r\"\\s+\", \" \", s).strip()\n","\n","# ==========================================\n","# WER COMPUTATION LOOP\n","# ==========================================\n","for label, path in FILES.items():\n","    print(f\"\\n=== Computing WER for {label.upper()} ===\")\n","    df = pd.read_csv(path)\n","\n","    if \"transcription\" not in df.columns:\n","        raise ValueError(f\"No transcription column found in {path}\")\n","\n","    for model in [\"whisper_small\", \"wav2vec2\", \"conformer\"]:\n","        if model not in df.columns:\n","            print(f\"⚠️ Column {model} not found in {label}, skipping.\")\n","            continue\n","\n","        wer_col = f\"{model}_wer\"\n","        print(f\"  → Calculating {wer_col}\")\n","\n","        df[wer_col] = df.apply(\n","            lambda row: min(\n","                1.0,\n","                wer(normalize(row[\"transcription\"]), normalize(row[model]))\n","            ),\n","            axis=1\n","        )\n","\n","    # Save WER results to a new file\n","    out_path = path.replace(\".csv\", \"_wer.csv\")\n","    df.to_csv(out_path, index=False)\n","    print(f\"✅ Saved WER results → {out_path}\")"],"metadata":{"id":"-dOUEQg4ud0W"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### WER Output Interpretation\n","\n","Values range from **0.0 (perfect match)** to **1.0 (completely incorrect)**.  \n","Because individual utterances can vary widely in difficulty, per-file WER offers fine-grained insight into where models fail.  \n","\n","The average Word Error Rates (WER) for each model and dataset are summarized below:\n","\n","| Dataset | Whisper-Small | Wav2Vec2-Large | Conformer-CTC |\n","|:---------|:--------------:|:---------------:|:---------------:|\n","| **CSLU Kids’ Speech** | **0.3938** | 0.7329 | 0.6450 |\n","| **TomRoma Child Speech** | **0.2321** | 0.3858 | 0.3805 |\n","\n","**Interpretation:**  \n","- Whisper-Small achieved the lowest WER across both datasets, consistent with its multilingual and robust pretraining.  \n","- Wav2Vec2 and Conformer exhibited higher errors, particularly on the CSLU corpus, which contains longer and more articulated utterances.  \n","- The TomRoma dataset yielded generally lower WERs, likely due to shorter, simpler phrases and cleaner background conditions."],"metadata":{"id":"8W3UwKnguwhF"}}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"authorship_tag":"ABX9TyOWm1B4SD5rqoh5As2Pzbit"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}