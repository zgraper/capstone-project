{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e98037b6-6fcb-4b46-82f5-3deadaae8bf4",
   "metadata": {},
   "source": [
    "# **Fining-tuning T5 Seq2Seq Model**\n",
    "\n",
    "Zane Graper\n",
    "\n",
    "MSAI699 Capstone\n",
    "\n",
    "---\n",
    "\n",
    "This notebook fine-tunes a T5 sequence-to-sequence model to translate IPA phoneme sequences into natural language text, with an emphasis on improving performance on real child-speech patterns. To achieve this, we combine a large corpus of clean adult speech with the CHILDES-derived child-speech dataset, and then rigorously clean, normalize, and filter both sources to ensure lexical quality and remove explicit or unsuitable content. After constructing a balanced training/evaluation split, the notebook applies the standard T5 preprocessing pipeline, trains the model for several epochs with carefully tuned hyperparameters, and evaluates performance using CER, BLEU, and chrF—metrics commonly used in phoneme-to-text research. The final sections save and package the trained model for downstream deployment and reproducibility.\n",
    "\n",
    "---\n",
    "\n",
    "### Install Dependencies and GPU Check\n",
    "\n",
    "This block installs all required Python packages—Transformers, Datasets, Accelerate, Evaluate, and additional utilities—ensuring the environment matches the expected versions for stable training. It also verifies that CUDA is available so GPU acceleration is enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "089e52ad-654c-47fa-bd7e-28ea14aed026",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /usr/local/lib/python3.12/dist-packages (25.3)\n",
      "PyTorch version: 2.8.0+cu128\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "\n",
    "# Core NLP + Transformers stack\n",
    "!pip install -q transformers==4.40.2\n",
    "!pip install -q datasets==2.19.0\n",
    "!pip install -q accelerate==0.30.1\n",
    "!pip install -q evaluate==0.4.2\n",
    "!pip install -q sentencepiece\n",
    "!pip install -q sacrebleu\n",
    "\n",
    "# Metrics\n",
    "!pip install -q jiwer\n",
    "\n",
    "# Data handling\n",
    "!pip install -q pandas\n",
    "!pip install -q numpy\n",
    "\n",
    "# Optional (helps with large dataset streaming)\n",
    "!pip install -q pyarrow\n",
    "\n",
    "# Optional: progress bars\n",
    "!pip install -q tqdm\n",
    "\n",
    "!pip install -q hf_transfer\n",
    "\n",
    "# Safety check: verify GPU availability\n",
    "import torch\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b71717-6732-4fb4-984a-54fd02476778",
   "metadata": {},
   "source": [
    "### Configuration Setup\n",
    "\n",
    "This section defines core configuration values, including paths to the perfect-speech and CHILDES data, train/eval split ratios, maximum text lengths, and hyperparameters for data cleaning. These variables allow reproducible control over corpus composition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e469b0e9-ba71-4e02-9826-f854f6d4520e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading perfect-speech dataset...\n",
      "Normalizing perfect corpus...\n",
      "Loading child-speech datasets...\n",
      "Perfect speech samples (raw): 788,370\n",
      "CHILDES samples (raw): 184,171\n",
      "\n",
      "Cleaning perfect corpus...\n",
      "Filtered explicit/violent content: removed 16,427 lines\n",
      "Filtered long lines: removed 829 lines\n",
      "\n",
      "Cleaning CHILDES corpus...\n",
      "Filtered explicit/violent content: removed 562 lines\n",
      "Filtered long lines: removed 56 lines\n",
      "\n",
      "Perfect after cleaning: 771,045\n",
      "CHILDES after cleaning: 183,553\n",
      "\n",
      "Desired perfect samples: 428,290\n",
      "Using perfect samples: 428,290\n",
      "Using child samples: 183,553\n",
      "Merged dataset size (cleaned): 611,843\n",
      "Training set size: 550,658\n",
      "Evaluation set size: 61,185\n",
      "\n",
      "Saved cleaned corpora:\n",
      " - train_merged.csv\n",
      " - eval_merged.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# ===========================================================\n",
    "# CONFIGURATION\n",
    "# ===========================================================\n",
    "PERFECT_PATH = \"bookcorpus_ipa_final.csv\"\n",
    "CHILD_TRAIN_PATH = \"child_train.tsv\"\n",
    "CHILD_VALID_PATH = \"child_valid.tsv\"\n",
    "\n",
    "# Ratio perfect : child in merged dataset (e.g., 0.7 = 70% perfect, 30% child)\n",
    "RATIO_PERFECT = 0.70\n",
    "\n",
    "# Train/eval split\n",
    "TRAIN_SPLIT = 0.90\n",
    "\n",
    "# Max allowed text length (tokens or chars)\n",
    "MAX_TEXT_LEN = 300\n",
    "MAX_IPA_LEN = 300\n",
    "\n",
    "# ===========================================================\n",
    "# ADULT CONTENT FILTERS\n",
    "# ===========================================================\n",
    "ADULT_WORDS = [\n",
    "    # sexual content\n",
    "    r\"\\bsex\\b\", r\"\\bsexual\\b\", r\"\\bthrust\\b\", r\"\\bmoan\", r\"\\bpleasure\\b\",\n",
    "    r\"\\bnaked\\b\", r\"\\bbreasts?\\b\", r\"\\bclit\\b\", r\"\\bpenis\\b\", r\"\\bvagina\\b\",\n",
    "    r\"\\bordered\\b\", r\"\\borgasm\\b\", r\"\\berotic\\b\", r\"\\bcondom\\b\",\n",
    "\n",
    "    # graphic or intimate touch\n",
    "    r\"\\btongue\\b\", r\"\\bwetly\\b\", r\"\\bcupping\\b\", r\"\\bswollen\\b\",\n",
    "\n",
    "    # violence (optional)\n",
    "    r\"\\bblood\\b\", r\"\\bstab\\b\", r\"\\bkill\\b\", r\"\\bknife\\b\",\n",
    "    r\"\\bmurder\\b\", r\"\\bslit\\b\"\n",
    "]\n",
    "\n",
    "def remove_explicit_text(df, text_col=\"text\"):\n",
    "    pattern = re.compile(\"|\".join(ADULT_WORDS), flags=re.IGNORECASE)\n",
    "    mask = ~df[text_col].str.contains(pattern, na=False)\n",
    "    removed = len(df) - mask.sum()\n",
    "    print(f\"Filtered explicit/violent content: removed {removed:,} lines\")\n",
    "    return df[mask]\n",
    "\n",
    "# ===========================================================\n",
    "# LENGTH FILTERS\n",
    "# ===========================================================\n",
    "def filter_length(df):\n",
    "    before = len(df)\n",
    "    df = df[\n",
    "        (df[\"text\"].str.len() <= MAX_TEXT_LEN) &\n",
    "        (df[\"phonemes\"].str.len() <= MAX_IPA_LEN)\n",
    "    ]\n",
    "    removed = before - len(df)\n",
    "    print(f\"Filtered long lines: removed {removed:,} lines\")\n",
    "    return df\n",
    "\n",
    "# ===========================================================\n",
    "# NORMALIZE TEXT (quotes, stray punctuation)\n",
    "# ===========================================================\n",
    "def clean_text(s):\n",
    "    if not isinstance(s, str):\n",
    "        return s\n",
    "    s = s.replace(\"''\", \"'\")\n",
    "    s = s.replace(\"  \", \" \")\n",
    "    return s.strip()\n",
    "\n",
    "def normalize_df(df):\n",
    "    df[\"text\"] = df[\"text\"].astype(str).apply(clean_text)\n",
    "    df[\"phonemes\"] = df[\"phonemes\"].astype(str).str.strip()\n",
    "    return df\n",
    "\n",
    "# ===========================================================\n",
    "# LOAD DATA\n",
    "# ===========================================================\n",
    "print(\"Loading perfect-speech dataset...\")\n",
    "df_perfect = pd.read_csv(PERFECT_PATH)\n",
    "\n",
    "print(\"Normalizing perfect corpus...\")\n",
    "df_perfect = normalize_df(df_perfect)\n",
    "\n",
    "print(\"Loading child-speech datasets...\")\n",
    "df_child_train = pd.read_csv(CHILD_TRAIN_PATH, sep=\"\\t\", header=None, names=[\"phonemes\", \"text\"])\n",
    "df_child_valid = pd.read_csv(CHILD_VALID_PATH, sep=\"\\t\", header=None, names=[\"phonemes\", \"text\"])\n",
    "\n",
    "df_child = pd.concat([df_child_train, df_child_valid], ignore_index=True)\n",
    "df_child = normalize_df(df_child)\n",
    "\n",
    "print(f\"Perfect speech samples (raw): {len(df_perfect):,}\")\n",
    "print(f\"CHILDES samples (raw): {len(df_child):,}\")\n",
    "\n",
    "# ===========================================================\n",
    "# CLEAN BOTH CORPORA\n",
    "# ===========================================================\n",
    "print(\"\\nCleaning perfect corpus...\")\n",
    "df_perfect = remove_explicit_text(df_perfect)\n",
    "df_perfect = filter_length(df_perfect)\n",
    "df_perfect = df_perfect.drop_duplicates()\n",
    "\n",
    "print(\"\\nCleaning CHILDES corpus...\")\n",
    "df_child = remove_explicit_text(df_child)\n",
    "df_child = filter_length(df_child)\n",
    "df_child = df_child.drop_duplicates()\n",
    "\n",
    "print(f\"\\nPerfect after cleaning: {len(df_perfect):,}\")\n",
    "print(f\"CHILDES after cleaning: {len(df_child):,}\")\n",
    "\n",
    "# ===========================================================\n",
    "# DETERMINE OPTIMAL COUNTS\n",
    "# ===========================================================\n",
    "child_n = len(df_child)\n",
    "perfect_required = int((RATIO_PERFECT / (1 - RATIO_PERFECT)) * child_n)\n",
    "\n",
    "perfect_available = len(df_perfect)\n",
    "perfect_n = min(perfect_required, perfect_available)\n",
    "\n",
    "print(f\"\\nDesired perfect samples: {perfect_required:,}\")\n",
    "print(f\"Using perfect samples: {perfect_n:,}\")\n",
    "print(f\"Using child samples: {child_n:,}\")\n",
    "\n",
    "# ===========================================================\n",
    "# SAMPLE DATASETS\n",
    "# ===========================================================\n",
    "df_perfect_sampled = df_perfect.sample(perfect_n, random_state=42)\n",
    "df_child_sampled = df_child  # keep all child speech\n",
    "\n",
    "df_merged = pd.concat([df_perfect_sampled, df_child_sampled], ignore_index=True)\n",
    "\n",
    "# Shuffle\n",
    "df_merged = df_merged.sample(frac=1.0, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(f\"Merged dataset size (cleaned): {len(df_merged):,}\")\n",
    "\n",
    "# ===========================================================\n",
    "# TRAIN / EVAL SPLIT\n",
    "# ===========================================================\n",
    "train_size = int(TRAIN_SPLIT * len(df_merged))\n",
    "\n",
    "df_train = df_merged.iloc[:train_size]\n",
    "df_eval = df_merged.iloc[train_size:]\n",
    "\n",
    "print(f\"Training set size: {len(df_train):,}\")\n",
    "print(f\"Evaluation set size: {len(df_eval):,}\")\n",
    "\n",
    "# ===========================================================\n",
    "# SAVE OUTPUT\n",
    "# ===========================================================\n",
    "df_train.to_csv(\"train_merged.csv\", index=False)\n",
    "df_eval.to_csv(\"eval_merged.csv\", index=False)\n",
    "\n",
    "print(\"\\nSaved cleaned corpora:\")\n",
    "print(\" - train_merged.csv\")\n",
    "print(\" - eval_merged.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20e5e10-2bf1-4b52-ad8b-df7d6049c695",
   "metadata": {},
   "source": [
    "### Train/Eval Split and Save\n",
    "\n",
    "This section partitions the merged corpus into training and evaluation sets based on the configured proportion. It then saves the resulting CSV files for later steps in the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "829ac5ce-0028-4784-ba70-86f572803554",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Sampling 50 rows from train_merged.csv ===\n",
      "\n",
      "--- Sample 538855 ---\n",
      "IPA:  aɪ m ʌ s t h æ v h ə t h ɪ z l ɛ g b ɪ k ɔ z h i s u n g eɪ v ʌ p ð ʌ ʧ eɪ s\n",
      "Text: i must have hurt his leg because he soon gave up the chase '\n",
      "\n",
      "--- Sample 459816 ---\n",
      "IPA:  aɪ k ʊ d n ɑ t h ɛ l p g l æ n s ɪ ŋ oʊ v ə æ t eɪ n ʤ ʌ l\n",
      "Text: i could not help glancing over at angel\n",
      "\n",
      "--- Sample 505200 ---\n",
      "IPA:  s oʊ f ɑ r ð ɛ r w ɑ z n ʌ θ ɪ ŋ\n",
      "Text: so far there was nothing\n",
      "\n",
      "--- Sample 151642 ---\n",
      "IPA:  g oʊ t u ð ʌ f ɔ l z ʃ i k ʊ d f ɑ l oʊ ð ʌ s aʊ n d\n",
      "Text: go to the falls ' she could follow the sound\n",
      "\n",
      "--- Sample 75153 ---\n",
      "IPA:  aɪ b ɛ t aɪ s aʊ n d ʌ d l aɪ k ɛ v ə i ʌ ð ə t i n eɪ ʤ g ə l ɪ n ʌ m ɛ r ʌ k ʌ d r u l ɪ ŋ oʊ v ə d eɪ ʤ oʊ n z\n",
      "Text: i bet i sounded like every other teenage girl in america drooling over day jones\n",
      "\n",
      "--- Sample 512479 ---\n",
      "IPA:  f aɪ n ɪ t w ɑ z m i h i ʌ d m ɪ t ʌ d h æ ŋ ɪ ŋ h ɪ z h ɛ d\n",
      "Text: fine it was me ' he admitted hanging his head\n",
      "\n",
      "--- Sample 135677 ---\n",
      "IPA:  h i m eɪ h æ v l aɪ d ʌ b aʊ t h ɪ z p æ s t h ɪ d ʌ n ð æ t h i h æ z ɛ v ə b ɪ n æ n æ k ʧ ʌ w ɛ r i h i ʌ n d h ɪ z ʌ p ɔ l ɪ ŋ w aɪ f m eɪ h æ v b ɪ n ɪ n ɪ t t ʌ g ɛ ð ə\n",
      "Text: he may have lied about his past hidden that he has ever been an actuary he and his appalling wife may have been in it together\n",
      "\n",
      "--- Sample 210639 ---\n",
      "IPA:  w aɪ d u j u h æ v n aɪ t m ɛ r z\n",
      "Text: why do you have nightmares '\n",
      "\n",
      "--- Sample 72237 ---\n",
      "IPA:  b ʌ t ð æ t s n ɑ t m aɪ f ɹ ɛ n d\n",
      "Text: but that's not my friend!\n",
      "\n",
      "--- Sample 424328 ---\n",
      "IPA:  d ɪ d ʌ n t j u ʤ ʌ s t ɪ k s p l eɪ n t u m i ð æ t ð ʌ h æ p i ʌ s t d eɪ ɪ n j ɔ r l aɪ f w ɑ z w ɛ n j u l ɛ f t ð ʌ s ɪ t i f ɔ r s æ g h ɑ r b ə\n",
      "Text: didnt you just explain to me that the happiest day in your life was when you left the city for sag harbor\n",
      "\n",
      "--- Sample 335781 ---\n",
      "IPA:  s oʊ f i b l ɪ ŋ k t s t ʌ n d ɪ n t u ʌ t ə s aɪ l ʌ n s\n",
      "Text: sophie blinked stunned into utter silence\n",
      "\n",
      "--- Sample 455365 ---\n",
      "IPA:  æ n d ð ɪ s ɪ z ð ə d ɑ ɡ z k æ t\n",
      "Text: and this is the dog's cat.\n",
      "\n",
      "--- Sample 194069 ---\n",
      "IPA:  ð ɪ s w ɑ z s ɪ m p l i ð ʌ w eɪ ʌ v θ ɪ ŋ z\n",
      "Text: this was simply the way of things\n",
      "\n",
      "--- Sample 361689 ---\n",
      "IPA:  aɪ m ɡ ə n ə ʃ oʊ ð ɪ ʌ ð ə ɹ ɡ aɪ z\n",
      "Text: i'm gonna show the other guys.\n",
      "\n",
      "--- Sample 77220 ---\n",
      "IPA:  aɪ w ɪ l g ɛ t w ʌ n f r ʌ m m aɪ b r ʌ ð ə ɪ z r u m\n",
      "Text: i will get one from my brother is room '\n",
      "\n",
      "--- Sample 503510 ---\n",
      "IPA:  j u w ɑ n t ʌ d t u s i m i f i d d ɪ d n ɑ t j u\n",
      "Text: you wanted to see me feed did not you '\n",
      "\n",
      "--- Sample 218073 ---\n",
      "IPA:  ɪ n s aɪ d b ɪ g ɪ n ɪ ŋ t u f oʊ k ʌ s f r ʌ m ð ɛ r s ʌ d ʌ n ə aʊ z ʌ l h ə s t ɪ n t r ʌ d u s t ð ɛ m t u ʌ g r eɪ h ɛ r d m æ n s ɪ t ɪ ŋ ɪ n ð ʌ m ɪ d ʌ l ʌ v ð ʌ f ɔ r w ə d f eɪ s ɪ ŋ r ɪ r s i t\n",
      "Text: inside beginning to focus from their sudden arousal hurst introduced them to a grey haired man sitting in the middle of the forward facing rear seat\n",
      "\n",
      "--- Sample 101742 ---\n",
      "IPA:  h i w ʊ d ɔ l m oʊ s t d ʌ n b oʊ θ\n",
      "Text: he would almost done both\n",
      "\n",
      "--- Sample 444048 ---\n",
      "IPA:  m æ k s ɪ z d i p v ɔɪ s b u m d ɪ n s aɪ d ð ʌ ɪ n s t ɪ t u ʃ ʌ n ʌ l t s s t r ʌ k ʧ ə\n",
      "Text: maax is deep voice boomed inside the institutional style structure\n",
      "\n",
      "--- Sample 105257 ---\n",
      "IPA:  ð ɪ s w eɪ ɔ ɹ ð ɪ s w eɪ\n",
      "Text: this way or this way.\n",
      "\n",
      "--- Sample 86119 ---\n",
      "IPA:  ʃ i w ʊ d h i r ɛ v r i θ ɪ ŋ h i s ɛ d b ʌ t n oʊ w ə d ð æ t æ z s p oʊ k\n",
      "Text: she would hear everything he said but no word that az spoke\n",
      "\n",
      "--- Sample 455173 ---\n",
      "IPA:  h aʊ j uː m eɪ k ʌ s ɪ k s\n",
      "Text: how you make a six?\n",
      "\n",
      "--- Sample 50872 ---\n",
      "IPA:  d æ l ɪ n ə k ʊ d n ɑ t g ɛ t ʌ g ʊ d v j u ʌ n d s k aʊ t r ɪ p ɔ r t s w ə t eɪ k ɪ ŋ t u l ɔ ŋ\n",
      "Text: dalinar could not get a good view and scout reports were taking too long\n",
      "\n",
      "--- Sample 51973 ---\n",
      "IPA:  ɪ z ð æ t w ʌ t ɪ z h æ p ʌ n ɪ ŋ\n",
      "Text: is that what is happening '\n",
      "\n",
      "--- Sample 303408 ---\n",
      "IPA:  h ɪ z w ɑ z n ɑ t ʌ n eɪ m ð æ t w ɑ z s p oʊ k ʌ n ɔ f ʌ n ɪ n aʊ ə h aʊ s h oʊ l d\n",
      "Text: his was not a name that was spoken often in our household\n",
      "\n",
      "--- Sample 138769 ---\n",
      "IPA:  ɛ n oʊ ʧ i ʌ t ʊ k ʌ s ɪ p ʌ v h ə w aɪ n ʌ n d s l oʊ l i s ɛ t ɪ t d aʊ n\n",
      "Text: enochia took a sip of her wine and slowly set it down\n",
      "\n",
      "--- Sample 243008 ---\n",
      "IPA:  ð ʌ eɪ ʤ ʌ n s i i v ɪ n f aʊ n d æ n ʌ n k l eɪ m d b ɑ d i æ t ð ʌ m ɔ r g ð æ t l ʊ k t ɪ n ʌ f l aɪ k m i ʌ n d w ɪ ð s ʌ ʧ ɪ k s t ɛ n s ɪ v ɪ n ʤ ə i z t u ð æ t b ɑ d i n oʊ w ʌ n l ʊ k t t u k l oʊ s ɔ r k w ɛ s ʧ ʌ n d\n",
      "Text: the agency even found an unclaimed body at the morgue that looked enough like me and with such extensive injuries to that body no one looked too close or questioned\n",
      "\n",
      "--- Sample 476955 ---\n",
      "IPA:  h ə b r aʊ n aɪ z ʃ ɪ f t ʌ d m ɛ l t ɪ ŋ ɪ n t u ʌ w ɔ r m ʃ eɪ d ʌ v ʧ ɔ k l ʌ t\n",
      "Text: her brown eyes shifted melting into a warm shade of chocolate\n",
      "\n",
      "--- Sample 71476 ---\n",
      "IPA:  f ɔ r ʌ m oʊ m ʌ n t aɪ s t æ n d ð ɛ r ʌ n d w ɑ ʧ ʌ b ə g ə r æ p ə b l oʊ ʌ k r ɔ s ð ʌ p ɑ r k ɪ ŋ l ɑ t\n",
      "Text: for a moment i stand there and watch a burger wrapper blow across the parking lot\n",
      "\n",
      "--- Sample 47493 ---\n",
      "IPA:  oʊ w iː h æ v t ə t eɪ k d æ t aʊ t\n",
      "Text: oh we have to take dat out!\n",
      "\n",
      "--- Sample 34252 ---\n",
      "IPA:  s oʊ h i t ʊ k h ɪ z t aɪ m ʌ n b aɪ n d ɪ ŋ h ə\n",
      "Text: so he took his time unbinding her\n",
      "\n",
      "--- Sample 329079 ---\n",
      "IPA:  h ɑ l i oʊ p ʌ n d h ɪ z aɪ z ð ɛ n ʌ n d l ʊ k t ʌ p æ t r ɪ ʤ v eɪ g l i\n",
      "Text: holley opened his eyes then and looked up at ridge vaguely\n",
      "\n",
      "--- Sample 25542 ---\n",
      "IPA:  ð ʌ h æ n d ɑ n l u i s ɪ z b æ k h æ d l ɛ f t l u i s ɪ z m aʊ ð w aɪ d oʊ p ʌ n\n",
      "Text: the hand on luis is back had left luis is mouth wide open\n",
      "\n",
      "--- Sample 37577 ---\n",
      "IPA:  ð ɛ r ɑ r ð ʌ g æ r ɪ s ʌ n z h i r ʌ n d ð ɛ r aɪ s ʌ p oʊ z b ʌ t ɔ l ʌ v ð ʌ s t æ n d ɪ ŋ r ɛ ʤ ʌ m ʌ n t s h æ v b ɪ n r ɪ k ɔ l d t u ɪ ŋ g l ʌ n d ɔ r s k ɑ t l ʌ n d\n",
      "Text: there are the garrisons here and there i suppose but all of the standing regiments have been recalled to england or scotland\n",
      "\n",
      "--- Sample 195271 ---\n",
      "IPA:  j u v ʌ k r ɪ t ɪ k ʌ l b r eɪ n j u s ɪ t\n",
      "Text: youve a critical brain use it\n",
      "\n",
      "--- Sample 530666 ---\n",
      "IPA:  l ɛ t s s iː h oʊ l w iː t b ɹ ɛ d\n",
      "Text: let's see whole wheat bread.\n",
      "\n",
      "--- Sample 549286 ---\n",
      "IPA:  b l æ k ʃ ɪ t ə d ɑ r k g l æ s ʌ z ʃ eɪ v d h ɛ d\n",
      "Text: black t shirt dark glasses shaved head\n",
      "\n",
      "--- Sample 173566 ---\n",
      "IPA:  t r aɪ ɪ ŋ æ z ɪ t m eɪ b i aɪ n u ʃ i k ʊ d d u ɪ t\n",
      "Text: trying as it may be i knew she could do it\n",
      "\n",
      "--- Sample 151569 ---\n",
      "IPA:  θ æ ŋ k j uː j æ b ɑ ɹ d æ b ɑ ɹ æ b ɑ ɹ d æ b ɑ ɹ æ b ɑ ɹ d æ b ɑ ɹ\n",
      "Text: thank you yabbar dabbar abbar dabbar abbar dabbar!\n",
      "\n",
      "--- Sample 549172 ---\n",
      "IPA:  j uː w ɔ n t t ɑ s iː d ɪ s ɛ ɪ n t ɛ l ɪ d̠ʒ ə n t s l aɪ d z\n",
      "Text: you want ta see dese intelligent slides?\n",
      "\n",
      "--- Sample 498978 ---\n",
      "IPA:  h i k ɛ p t w ɔ k ɪ ŋ ʌ w eɪ f r ʌ m m i\n",
      "Text: he kept walking away from me\n",
      "\n",
      "--- Sample 224819 ---\n",
      "IPA:  f ɜː s t j ɛ h f ɜː s t w ʌ n d aɪ d\n",
      "Text: first yeah first one died.\n",
      "\n",
      "--- Sample 394708 ---\n",
      "IPA:  ð ʌ m æ s ɪ v ʃ ɪ p n ɪ r l i ɪ n g ʌ l f t ð ʌ s ɪ t i s k aɪ l aɪ n\n",
      "Text: the massive ship nearly engulfed the city skyline\n",
      "\n",
      "--- Sample 424491 ---\n",
      "IPA:  aɪ d ɪ d n t f aɪ n d ʌ s t oʊ n\n",
      "Text: i didn't find a stone.\n",
      "\n",
      "--- Sample 15829 ---\n",
      "IPA:  w ɛ l w ɪ l j uː t eɪ k ð ə b ɔ l\n",
      "Text: well will you take the ball.\n",
      "\n",
      "--- Sample 304773 ---\n",
      "IPA:  v ɪ k i d r ɑ p t ð ʌ m ɪ s t ɪ r i ʌ s b æ g ɑ n ð ʌ k aʊ ʧ æ z ʃ i h ɛ d ʌ d t u ð ʌ b æ θ r u m t u w ɑ ʃ h ə f eɪ s\n",
      "Text: vicky dropped the mysterious bag on the couch as she headed to the bathroom to wash her face\n",
      "\n",
      "--- Sample 409473 ---\n",
      "IPA:  aɪ p r ɪ z u m h i ʤ ʌ s t w ɔ k t ɪ n\n",
      "Text: i presume he just walked in\n",
      "\n",
      "--- Sample 446732 ---\n",
      "IPA:  h i ʌ s u m d ð eɪ k eɪ m f ɔ r w ʌ n r i ʌ s ɑ n t oʊ k ʌ m p l i t l i d ɪ m ɑ l ɪ ʃ h ɪ m\n",
      "Text: he assumed they came for one reason to completely demolish him\n",
      "\n",
      "--- Sample 56819 ---\n",
      "IPA:  p r ɑ m ʌ s m i ʌ g ɛ n ð æ t j u w oʊ n ɑ t b i w ə k ɪ ŋ l eɪ t m ʌ ʧ l ɔ ŋ g ə\n",
      "Text: promise me again that you wo not be working late much longer '\n",
      "\n",
      "--- Sample 497928 ---\n",
      "IPA:  j u h æ v b ɪ n ɪ n h aɪ d ɪ ŋ l oʊ g ʌ n s ɛ z æ z h i s ɪ t s n ɛ k s t t u m i ʌ d r ɪ ŋ k k l ʌ ʧ t ɪ n h ɪ z h æ n d\n",
      "Text: you have been in hiding ' logan says as he sits next to me a drink clutched in his hand\n",
      "\n",
      "=== Sampling 50 rows from eval_merged.csv ===\n",
      "\n",
      "--- Sample 38417 ---\n",
      "IPA:  aɪ m iː n æ n d æ f t ə ɹ ð æ t h iː l ɛ f t m ʌ t̠ʃ b ɛ t ə ɹ\n",
      "Text: i mean and after that he left much better.\n",
      "\n",
      "--- Sample 42742 ---\n",
      "IPA:  w ʌ t ɑ r j u p l æ n ɪ ŋ ɑ n d u ɪ ŋ\n",
      "Text: what are you planning on doing\n",
      "\n",
      "--- Sample 9425 ---\n",
      "IPA:  ʌ l ɑ r ʤ f aʊ n t ʌ n s æ t ʌ l ɔ ŋ w ʌ n w ɔ l w ɛ r ʌ t eɪ b ʌ l w ɪ ð f ɔ r ʧ ɛ r z s æ t s ə aʊ n d ʌ d b aɪ ʌ l ʌ ʃ g ɑ r d ʌ n ʌ v v aɪ n z ʌ n d f l aʊ ə z ʌ n d ʌ s m ɔ l v ɛ ʤ t ʌ b ʌ l g ɑ r d ʌ n ɔ f t u ð ʌ r aɪ t\n",
      "Text: a large fountain sat along one wall where a table with four chairs sat surrounded by a lush garden of vines and flowers and a small vegetable garden off to the right\n",
      "\n",
      "--- Sample 28418 ---\n",
      "IPA:  k l æ r ʌ k r aɪ d aʊ t s k ɔ r ɪ ŋ h ɪ z b æ k æ z h i t ʊ k h ə oʊ v ə ð ʌ ɛ ʤ\n",
      "Text: clara cried out scoring his back as he took her over the edge\n",
      "\n",
      "--- Sample 2336 ---\n",
      "IPA:  h i ɪ z m aɪ p æ s t ʌ n d aɪ n i d t u s t ɑ r t f oʊ k ʌ s ɪ ŋ ɑ n w ʌ t aɪ æ m g oʊ ɪ ŋ t u d u ʌ b aʊ t m aɪ f j u ʧ ə\n",
      "Text: he is my past and i need to start focusing on what i am going to do about my future '\n",
      "\n",
      "--- Sample 40926 ---\n",
      "IPA:  ɪ z r i ʌ l ɪ z r oʊ l ɪ ŋ ʌ w eɪ f r ʌ m m i b aʊ n s ɪ ŋ ɑ n ð ʌ h j u ʤ ɪ n f l eɪ t ʌ d p æ d ʌ n d ɑ n t u ð ʌ g r aʊ n d\n",
      "Text: israel is rolling away from me bouncing on the huge inflated pad and onto the ground\n",
      "\n",
      "--- Sample 34157 ---\n",
      "IPA:  j uː ɡ ɑ t ə l ɪ k ɪ t ɹ aɪ t\n",
      "Text: you gotta lick it right?\n",
      "\n",
      "--- Sample 24741 ---\n",
      "IPA:  t eɪ k ð oʊ z t u aʊ t ʌ n d k i p ð ɛ m b ɪ h aɪ n d ʌ b æ r i ə aʊ t s aɪ d w ʌ n ɑ n i ʧ s aɪ d ʌ v ð ʌ k ɑ m p aʊ n d\n",
      "Text: take those two out and keep them behind a barrier outside one on each side of the compound '\n",
      "\n",
      "--- Sample 37494 ---\n",
      "IPA:  aɪ d u n ɑ t n oʊ w aɪ j u ɑ r ɪ n t r ʌ s t ʌ d ɪ n m i f ɔ r w ʌ n\n",
      "Text: i do not know why you are interested in me for one\n",
      "\n",
      "--- Sample 23305 ---\n",
      "IPA:  ð ɪ s b ɑ t ʌ l l ʊ k s l aɪ k ʌ r ɑ k ʌ t\n",
      "Text: this bottle looks like a rocket\n",
      "\n",
      "--- Sample 34747 ---\n",
      "IPA:  aɪ θ ɔ t s oʊ t u s ə b ʌ t ɪ t ɪ z n ɑ t\n",
      "Text: i thought so too sir but it is not\n",
      "\n",
      "--- Sample 10793 ---\n",
      "IPA:  h i w ʊ d b ɪ n s oʊ k oʊ l d f ɔ r s oʊ l ɔ ŋ\n",
      "Text: he would been so cold for so long\n",
      "\n",
      "--- Sample 5128 ---\n",
      "IPA:  h i r ɪ m eɪ n d ð ɪ s w eɪ f ɔ r ʌ m oʊ m ʌ n t t eɪ k ɪ ŋ ɪ n æ z m ʌ ʧ ʌ v d ɑ n i æ z h i k ʊ d ð ɛ n j æ ŋ k t d ɑ n i ɪ z b ɑ k s ə ʃ ɔ r t s d aʊ n h ɪ z l ɛ g z ʌ n d h ɛ l p t h ɪ m s t ɛ p aʊ t ʌ v ð ɛ m\n",
      "Text: he remained this way for a moment taking in as much of donny as he could then yanked donny is boxer shorts down his legs and helped him step out of them\n",
      "\n",
      "--- Sample 49497 ---\n",
      "IPA:  aɪ æ m t u b ɪ z i b i ɪ ŋ ɪ n p eɪ n\n",
      "Text: i am too busy being in pain '\n",
      "\n",
      "--- Sample 26148 ---\n",
      "IPA:  w i m eɪ d ɪ t t u ð ʌ ʧ ɪ m p s ɪ n ð ʌ l eɪ t æ f t ə n u n ʌ n d r ʌ b ɛ k ʌ s æ t ɪ n h ə k ɔ r n ə w ɪ ð h ə b l u b l æ ŋ k ʌ t ʌ n t ɪ l ʃ i s ɔ ʌ s k ʌ m ɪ ŋ\n",
      "Text: we made it to the chimps in the late afternoon and rebecca sat in her corner with her blue blanket until she saw us coming\n",
      "\n",
      "--- Sample 29464 ---\n",
      "IPA:  ʧ æ p t ə t u p l eɪ ɪ ŋ w ɪ ð f aɪ ə k eɪ l j ʊ r p l eɪ ɪ ŋ w ɪ ð f aɪ ə ʃ ʊ r l i j u ʃ ʊ d ʌ v n oʊ n ð æ t k ɑ r m ʌ ɪ z ʌ b ɪ ʧ ʌ n d ɪ m ʤ ʌ s t g ɛ t ɪ ŋ s t ɑ r t ʌ d r i m aɪ n d m i n ɛ v ə t u g ɛ t ʃ ɑ t ʌ g ɛ n\n",
      "Text: chapter 2 playing with fire cale youre playing with fire surely you shouldve known that karma is a bitch and im just getting started remind me never to get shot again\n",
      "\n",
      "--- Sample 25127 ---\n",
      "IPA:  ɪ n m aɪ ɪ k s p ɪ r i ʌ n s t i b ɪ ŋ s ɛ d m ɛ n g oʊ t u f ɑ r g r eɪ t ə l ɛ ŋ k θ s t u ʌ v ɔɪ d w ʌ t ð eɪ f ɪ r ð æ n t u ʌ b t eɪ n w ʌ t ð eɪ d ɪ z aɪ ə\n",
      "Text: in my experience ' teabing said ' men go to far greater lengths to avoid what they fear than to obtain what they desire\n",
      "\n",
      "--- Sample 57649 ---\n",
      "IPA:  ʃ i s k aʊ l d ʌ l ɪ t ʌ l m ɔ r ʌ n d t r aɪ d t u t ʌ g ð ʌ k ʌ m f ə t ə ə aʊ n d h ə b ʌ t d ɪ d n ɑ t s ʌ k s i d\n",
      "Text: she scowled a little more and tried to tug the comforter around her but did not succeed\n",
      "\n",
      "--- Sample 48805 ---\n",
      "IPA:  aɪ m eɪ k t u s w ɪ ŋ m aɪ s ɛ l f d aʊ n s oʊ aɪ k æ n p r ɑ p ə l i g r i t h ɪ m b ʌ t s t ɑ p w ɛ n t u l ɑ r ʤ ə g aɪ z s t ɛ p ɑ n t u ð ʌ p æ θ n ɛ k s t t u h ɪ m\n",
      "Text: i make to swing myself down so i can properly greet him but stop when two larger guys step onto the path next to him\n",
      "\n",
      "--- Sample 17726 ---\n",
      "IPA:  h ɪ z l ɪ p s t w ɪ s t ʌ d æ z h ɪ z g eɪ z f l æ ʃ t oʊ v ə h ə\n",
      "Text: his lips twisted as his gaze flashed over her\n",
      "\n",
      "--- Sample 61090 ---\n",
      "IPA:  ɪ m k ʌ m p l i t l i æ t ʌ l ɔ s æ z t u w ʌ t w i ʃ ʊ d d u\n",
      "Text: im completely at a loss as to what we should do\n",
      "\n",
      "--- Sample 40153 ---\n",
      "IPA:  ð eɪ w ə n ɑ t k r æ m d ɪ n æ t ɑ d æ ŋ g ʌ l z\n",
      "Text: they were not crammed in at odd angles\n",
      "\n",
      "--- Sample 22610 ---\n",
      "IPA:  s æ l d u j u n i d j ɔ r h æ n d h ɛ l d\n",
      "Text: sal do you need your hand held '\n",
      "\n",
      "--- Sample 17927 ---\n",
      "IPA:  aɪ l i n f ɔ r w ə d ʌ n d k ɪ s ð ʌ s m aɪ l ð æ t ʤ ʌ s t s p r ɛ d ʌ k r ɔ s h ə l ɪ p s æ z aɪ s aɪ l ʌ n t l i θ æ ŋ k ð ʌ j u n ʌ v ə s f ɔ r s ɛ n d ɪ ŋ h ə b æ k t u m i\n",
      "Text: i lean forward and kiss the smile that just spread across her lips as i silently thank the universe for sending her back to me\n",
      "\n",
      "--- Sample 57904 ---\n",
      "IPA:  m ɑ m i h aʊ d uː j uː m eɪ k ð ɪ s\n",
      "Text: mommy how do you make this?\n",
      "\n",
      "--- Sample 33394 ---\n",
      "IPA:  m aɪ n m aɪ n m aɪ n ʃ ɛ ɹ b eɪ b i z t uː n ɔɪ z\n",
      "Text: mine mine mine share babies too noise.\n",
      "\n",
      "--- Sample 2286 ---\n",
      "IPA:  aɪ f ɛ l t ʌ r ʌ ʃ ʌ v f ʌ m ɪ l j ɛ r ʌ t i æ z h ə w ə d z t ɪ k ʌ l d ð ʌ b æ k ʌ v m aɪ m aɪ n d\n",
      "Text: i felt a rush of familiarity as her words tickled the back of my mind\n",
      "\n",
      "--- Sample 24606 ---\n",
      "IPA:  d ʌ z ð æ t m iː n f ɔ ɹ b ɹ ɛ k f ə s t\n",
      "Text: does that mean for breakfast?\n",
      "\n",
      "--- Sample 40314 ---\n",
      "IPA:  ð ɛ n ʃ i s l æ m d h ə h æ n d θ r u ð ʌ g l æ s\n",
      "Text: then she slammed her hand through the glass\n",
      "\n",
      "--- Sample 38800 ---\n",
      "IPA:  l ɛ t ɪ z g ɛ t ð ʌ h ɛ l aʊ t ʌ v h i r\n",
      "Text: let is get the hell out of here\n",
      "\n",
      "--- Sample 58697 ---\n",
      "IPA:  g eɪ t ə k aʊ n t ʌ d t u f aɪ v ʌ n d p r ʌ p ɛ l d h ɪ z b ɑ d i f ɔ r w ə d θ r u ð ʌ m ʌ d j u z ɪ ŋ h ɪ z ɛ l b oʊ z\n",
      "Text: gator counted to five and propelled his body forward through the mud using his elbows\n",
      "\n",
      "--- Sample 53066 ---\n",
      "IPA:  k ɛ p t ʌ s ʌ l ɑ t m ɔ ɹ b ɪ z i ə ɹ\n",
      "Text: kept us a lot more busier.\n",
      "\n",
      "--- Sample 33035 ---\n",
      "IPA:  t ʌ n aɪ t aɪ w ɑ n t ʌ d t u b i f r i ʌ v ð ʌ ɛ k r oʊ p ɛ n s t ʌ n t eɪ k ɪ n m aɪ ʧ ɛ s t\n",
      "Text: tonight i wanted to be free of the ever constant ache in my chest\n",
      "\n",
      "--- Sample 4844 ---\n",
      "IPA:  æ n d ʃ iː ɡ oʊ z w ɑ ʃ h ɜː h æ n d z æ n d s t ʌ f\n",
      "Text: and she goes wash her hands and stuff.\n",
      "\n",
      "--- Sample 23772 ---\n",
      "IPA:  w ʌ t aɪ ʤ ʌ s t s ɛ d j u m eɪ b i j u ʌ n d m i m eɪ b i ð ɪ s ɪ z n ɑ t ð ʌ r aɪ t t aɪ m ʌ n d h i s t ɑ r t ʌ d ʧ ʌ k l ɪ ŋ\n",
      "Text: what i just said you maybe you and me maybe this is not the right time and ' he started chuckling\n",
      "\n",
      "--- Sample 59340 ---\n",
      "IPA:  g ʊ d l ʌ k w ɪ ð ð æ t\n",
      "Text: good luck with that '\n",
      "\n",
      "--- Sample 31549 ---\n",
      "IPA:  h i l i n d f ɔ r w ə d ʌ n d k ɪ s t h ə f ə m l i ɑ n ð ʌ l ɪ p s\n",
      "Text: he leaned forward and kissed her firmly on the lips\n",
      "\n",
      "--- Sample 44165 ---\n",
      "IPA:  ʌ v k ɔ r s ð æ t ɪ z ð ʌ ɪ m p ɔ r t ʌ n t θ ɪ ŋ b r ɛ n ʌ ʌ g r i d\n",
      "Text: of course that is the important thing ' brenna agreed\n",
      "\n",
      "--- Sample 60327 ---\n",
      "IPA:  s oʊ h iː ɡ ɛ t s t ə s oʊ t ə m ɑ ɹ oʊ m ɔ ɹ n ɪ ŋ aɪ m ɡ oʊ ɪ ŋ t ə k iː p h ɪ m ɪ n s aɪ d ð ə b ɑ ɹ n oʊ k eɪ\n",
      "Text: so he gets to so tomorrow morning i'm going to keep him inside the barn okay?\n",
      "\n",
      "--- Sample 23558 ---\n",
      "IPA:  m eɪ b iː ð eɪ ə ɹ ð ə b eɪ b i w ʌ n z\n",
      "Text: maybe they're the baby ones.\n",
      "\n",
      "--- Sample 3406 ---\n",
      "IPA:  aɪ ʃ ʊ d p ʊ t t uː h æ n d ɪ n ɪ t\n",
      "Text: i should put two hand in it.\n",
      "\n",
      "--- Sample 12253 ---\n",
      "IPA:  ʃ i l ʊ k t oʊ v ə h ə ʃ oʊ l d ə\n",
      "Text: she looked over her shoulder\n",
      "\n",
      "--- Sample 43109 ---\n",
      "IPA:  s oʊ w ʌ t d ɪ d j u t u d u t ʌ d eɪ\n",
      "Text: so what did you two do today '\n",
      "\n",
      "--- Sample 25882 ---\n",
      "IPA:  b ʌ t aɪ g ɛ s ɪ t ɔ l w ə k t aʊ t b ɪ k ɔ z aɪ n i d ʌ d m aɪ ʌ ð ə ʤ ɑ b\n",
      "Text: but i guess it all worked out because i needed my other job '\n",
      "\n",
      "--- Sample 41643 ---\n",
      "IPA:  h i h æ z g ʊ d ɪ n t ɛ n ʧ ʌ n z f ɔ r ð ʌ s ə v aɪ v ʌ l ʌ v h ɪ z p i p ʌ l t r aɪ ɪ ŋ t u s eɪ v w ʌ t m aɪ t r ɪ m eɪ n ʌ v h ɪ z h oʊ m l æ n d æ f t ə ð ʌ ɪ n v eɪ ʒ ʌ n\n",
      "Text: he has good intentions for the survival of his people trying to save what might remain of his homeland after the invasion\n",
      "\n",
      "--- Sample 54284 ---\n",
      "IPA:  h i d ɪ d n ɑ t r ɪ s p ɑ n d ʌ ð ə ð æ n t u ʃ r ʌ g\n",
      "Text: he did not respond other than to shrug\n",
      "\n",
      "--- Sample 24970 ---\n",
      "IPA:  t ɛ n d r ʌ l z ʌ v m ɪ s t d r ɪ f t ʌ d f r ʌ m h ə n ɑ s t r ʌ l z\n",
      "Text: tendrils of mist drifted from her nostrils\n",
      "\n",
      "--- Sample 7657 ---\n",
      "IPA:  w ɪ ð m aɪ ʌ m d ɹ ɛ s ɔ n\n",
      "Text: with my um dress on.\n",
      "\n",
      "--- Sample 55863 ---\n",
      "IPA:  ð ɛ n ð ɛ r w ɑ z ʌ s ʌ t ʌ l s aʊ n d l aɪ k f æ b r ɪ k b i ɪ ŋ t ɔ r n w ɪ ʧ s i m d t u g oʊ ɑ n ʌ n d ɑ n\n",
      "Text: then there was a subtle sound like fabric being torn which seemed to go on and on\n",
      "\n",
      "--- Sample 43126 ---\n",
      "IPA:  ð ɛ r ð ɛ r ɪ t s n ɑ t ð ʌ h oʊ l w ə l d\n",
      "Text: there there its not the whole world\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "TRAIN_PATH = \"train_merged.csv\"\n",
    "EVAL_PATH = \"eval_merged.csv\"\n",
    "\n",
    "# Number of random rows to preview\n",
    "N = 50\n",
    "\n",
    "def inspect_file(path, n=N):\n",
    "    print(f\"\\n=== Sampling {n} rows from {path} ===\")\n",
    "    df = pd.read_csv(path)\n",
    "    \n",
    "    # Ensure required columns exist\n",
    "    assert \"phonemes\" in df.columns, \"Missing column: phonemes\"\n",
    "    assert \"text\" in df.columns, \"Missing column: text\"\n",
    "    \n",
    "    # Sample without replacement\n",
    "    sample = df.sample(n=min(n, len(df)), random_state=np.random.randint(0, 1e6))\n",
    "\n",
    "    for i, row in sample.iterrows():\n",
    "        print(f\"\\n--- Sample {i} ---\")\n",
    "        print(\"IPA: \", row[\"phonemes\"])\n",
    "        print(\"Text:\", row[\"text\"])\n",
    "\n",
    "# Inspect both files\n",
    "inspect_file(TRAIN_PATH)\n",
    "inspect_file(EVAL_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3ec175e-6cb8-4288-9182-0c278c473125",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cleaned corpora...\n",
      "Train samples: 550,658\n",
      "Eval samples : 61,185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94f7cd4e168b45289ed4732751b262a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6290e70a12d041c780391ac15be0bd30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c607f0784f024d5da9ca014ce810e66e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c498c6508ae49a59b26d2a941c896a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0c1af698bfc4ff9a18b4cc065bfaf7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef25f7f467584d20bdc19aa25281207a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65d45f1e16634f979744959f50c2a82e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/550658 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16e79ac370574416a7d42d6ae8edd101",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/61185 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py:479: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25812' max='25812' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25812/25812 3:48:31, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>5.554000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>4.797700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>4.387800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>4.230100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>4.064800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>4.006100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>3.922700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>3.825500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>3.755000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>3.656100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>3.548100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>3.439300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>3.333500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>3.237600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>3.118500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>3.005700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>2.913700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>2.786300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>2.714100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.621900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>2.518900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>2.430900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>2.354400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>2.256200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>2.198500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>2.155400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>2.075700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>2.020800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>1.985900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.922900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>1.872900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>1.852700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>1.799300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>1.741900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>1.721500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>1.679400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>1.685600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>1.641800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>1.620700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.599400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>1.571600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>1.524700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>1.519200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>1.517000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>1.486600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>1.464300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>1.465700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>1.437300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>1.414100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>1.392900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>1.380000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>1.365500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>1.343200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>1.317500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>1.337500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>1.317700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>1.310800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>1.289100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>1.262300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>1.272500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6100</td>\n",
       "      <td>1.271700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>1.234800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6300</td>\n",
       "      <td>1.221900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>1.223800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>1.218800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>1.203400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6700</td>\n",
       "      <td>1.208400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>1.214500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6900</td>\n",
       "      <td>1.183900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>1.178600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7100</td>\n",
       "      <td>1.149800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>1.166400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7300</td>\n",
       "      <td>1.128400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>1.129200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>1.150700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>1.128000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7700</td>\n",
       "      <td>1.113800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>1.112900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7900</td>\n",
       "      <td>1.099400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>1.085900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8100</td>\n",
       "      <td>1.092800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>1.082800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8300</td>\n",
       "      <td>1.077800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>1.082700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>1.068800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8600</td>\n",
       "      <td>1.045800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8700</td>\n",
       "      <td>1.027600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8800</td>\n",
       "      <td>1.029100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8900</td>\n",
       "      <td>1.023000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>1.024800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9100</td>\n",
       "      <td>1.021200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9200</td>\n",
       "      <td>1.029300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9300</td>\n",
       "      <td>1.010700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9400</td>\n",
       "      <td>1.003000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.989900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9600</td>\n",
       "      <td>0.997800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9700</td>\n",
       "      <td>0.981200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9800</td>\n",
       "      <td>0.971700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9900</td>\n",
       "      <td>0.980700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.982900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10100</td>\n",
       "      <td>0.979800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10200</td>\n",
       "      <td>0.985500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10300</td>\n",
       "      <td>0.969100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10400</td>\n",
       "      <td>0.968700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.958400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10600</td>\n",
       "      <td>0.960500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10700</td>\n",
       "      <td>0.956900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10800</td>\n",
       "      <td>0.947300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10900</td>\n",
       "      <td>0.943700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.940300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11100</td>\n",
       "      <td>0.939200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11200</td>\n",
       "      <td>0.938900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11300</td>\n",
       "      <td>0.930100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11400</td>\n",
       "      <td>0.915700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.924600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11600</td>\n",
       "      <td>0.917100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11700</td>\n",
       "      <td>0.922800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11800</td>\n",
       "      <td>0.914000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11900</td>\n",
       "      <td>0.915500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.913600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12100</td>\n",
       "      <td>0.912700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12200</td>\n",
       "      <td>0.899900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12300</td>\n",
       "      <td>0.902400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12400</td>\n",
       "      <td>0.914700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.897900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12600</td>\n",
       "      <td>0.891100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12700</td>\n",
       "      <td>0.902000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12800</td>\n",
       "      <td>0.887900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12900</td>\n",
       "      <td>0.861900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.875100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13100</td>\n",
       "      <td>0.873000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13200</td>\n",
       "      <td>0.878600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13300</td>\n",
       "      <td>0.871600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13400</td>\n",
       "      <td>0.880800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.872400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13600</td>\n",
       "      <td>0.860400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13700</td>\n",
       "      <td>0.854500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13800</td>\n",
       "      <td>0.870100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13900</td>\n",
       "      <td>0.850200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.856600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14100</td>\n",
       "      <td>0.852100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14200</td>\n",
       "      <td>0.830800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14300</td>\n",
       "      <td>0.837700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14400</td>\n",
       "      <td>0.857100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>0.831900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14600</td>\n",
       "      <td>0.840500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14700</td>\n",
       "      <td>0.847700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14800</td>\n",
       "      <td>0.842300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14900</td>\n",
       "      <td>0.845900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.832900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15100</td>\n",
       "      <td>0.837500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15200</td>\n",
       "      <td>0.840200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15300</td>\n",
       "      <td>0.828700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15400</td>\n",
       "      <td>0.817700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>0.831600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15600</td>\n",
       "      <td>0.825600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15700</td>\n",
       "      <td>0.835100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15800</td>\n",
       "      <td>0.810900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15900</td>\n",
       "      <td>0.810000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.833900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16100</td>\n",
       "      <td>0.801200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16200</td>\n",
       "      <td>0.809400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16300</td>\n",
       "      <td>0.808100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16400</td>\n",
       "      <td>0.806900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>0.805800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16600</td>\n",
       "      <td>0.806100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16700</td>\n",
       "      <td>0.810700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16800</td>\n",
       "      <td>0.800600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16900</td>\n",
       "      <td>0.812900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.805600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17100</td>\n",
       "      <td>0.802700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17200</td>\n",
       "      <td>0.797500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17300</td>\n",
       "      <td>0.785300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17400</td>\n",
       "      <td>0.788300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>0.771600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17600</td>\n",
       "      <td>0.778700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17700</td>\n",
       "      <td>0.786600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17800</td>\n",
       "      <td>0.776700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17900</td>\n",
       "      <td>0.778900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.771500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18100</td>\n",
       "      <td>0.774200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18200</td>\n",
       "      <td>0.769600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18300</td>\n",
       "      <td>0.777400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18400</td>\n",
       "      <td>0.761900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>0.769600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18600</td>\n",
       "      <td>0.760100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18700</td>\n",
       "      <td>0.763800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18800</td>\n",
       "      <td>0.765000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18900</td>\n",
       "      <td>0.752800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.769500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19100</td>\n",
       "      <td>0.759000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19200</td>\n",
       "      <td>0.765700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19300</td>\n",
       "      <td>0.767700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19400</td>\n",
       "      <td>0.750900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>0.769800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19600</td>\n",
       "      <td>0.749600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19700</td>\n",
       "      <td>0.744400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19800</td>\n",
       "      <td>0.746700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19900</td>\n",
       "      <td>0.760100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.739100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20100</td>\n",
       "      <td>0.751100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20200</td>\n",
       "      <td>0.751800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20300</td>\n",
       "      <td>0.755000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20400</td>\n",
       "      <td>0.756900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20500</td>\n",
       "      <td>0.752700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20600</td>\n",
       "      <td>0.756000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20700</td>\n",
       "      <td>0.746900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20800</td>\n",
       "      <td>0.754500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20900</td>\n",
       "      <td>0.744200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.740200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21100</td>\n",
       "      <td>0.749200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21200</td>\n",
       "      <td>0.744000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21300</td>\n",
       "      <td>0.740800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21400</td>\n",
       "      <td>0.737200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21500</td>\n",
       "      <td>0.730900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21600</td>\n",
       "      <td>0.736900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21700</td>\n",
       "      <td>0.748400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21800</td>\n",
       "      <td>0.741600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21900</td>\n",
       "      <td>0.733700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.743900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22100</td>\n",
       "      <td>0.739100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22200</td>\n",
       "      <td>0.731700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22300</td>\n",
       "      <td>0.732400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22400</td>\n",
       "      <td>0.734500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>0.726700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22600</td>\n",
       "      <td>0.734400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22700</td>\n",
       "      <td>0.743600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22800</td>\n",
       "      <td>0.750100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22900</td>\n",
       "      <td>0.732900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.738900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23100</td>\n",
       "      <td>0.728100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23200</td>\n",
       "      <td>0.723000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23300</td>\n",
       "      <td>0.729700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23400</td>\n",
       "      <td>0.730700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23500</td>\n",
       "      <td>0.725700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23600</td>\n",
       "      <td>0.738000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23700</td>\n",
       "      <td>0.725300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23800</td>\n",
       "      <td>0.734400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23900</td>\n",
       "      <td>0.749600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.704300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24100</td>\n",
       "      <td>0.728000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24200</td>\n",
       "      <td>0.728300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24300</td>\n",
       "      <td>0.739400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24400</td>\n",
       "      <td>0.729400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24500</td>\n",
       "      <td>0.727900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24600</td>\n",
       "      <td>0.745900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24700</td>\n",
       "      <td>0.715600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24800</td>\n",
       "      <td>0.722400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24900</td>\n",
       "      <td>0.733000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.724200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25100</td>\n",
       "      <td>0.728400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25200</td>\n",
       "      <td>0.726000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25300</td>\n",
       "      <td>0.710200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25400</td>\n",
       "      <td>0.709600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25500</td>\n",
       "      <td>0.733100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25600</td>\n",
       "      <td>0.719900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25700</td>\n",
       "      <td>0.728700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25800</td>\n",
       "      <td>0.723100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ./t5_ipa_child_model\n",
      "Running final evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py:1141: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7649' max='7649' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7649/7649 24:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.544379472732544, 'eval_cer': 0.246944650000786, 'eval_bleu': 0.5811898999604037, 'eval_chrf': 70.53047702197442, 'eval_runtime': 2135.8609, 'eval_samples_per_second': 28.647, 'eval_steps_per_second': 3.581, 'epoch': 2.9999564162538315}\n",
      "Training results saved to training_results.txt\n"
     ]
    }
   ],
   "source": [
    "# ===============================================================\n",
    "# T5 IPA → TEXT TRAINING SCRIPT\n",
    "# (Aligned with methodology from prior notebook)\n",
    "# ===============================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "import evaluate\n",
    "cer = evaluate.load(\"cer\")\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "chrf = evaluate.load(\"chrf\")\n",
    "\n",
    "from transformers import (\n",
    "    T5Tokenizer,\n",
    "    T5ForConditionalGeneration,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments\n",
    ")\n",
    "\n",
    "# ===============================================================\n",
    "# CONFIGURATION\n",
    "# ===============================================================\n",
    "TRAIN_PATH = \"train_merged.csv\"\n",
    "EVAL_PATH  = \"eval_merged.csv\"\n",
    "OUTPUT_DIR = \"./t5_ipa_child_model\"\n",
    "\n",
    "MODEL_NAME = \"t5-small\"    # change to \"t5-medium\" if wanted and VRAM allows\n",
    "MAX_LENGTH = 256\n",
    "\n",
    "# ===============================================================\n",
    "# LOAD DATA\n",
    "# ===============================================================\n",
    "print(\"Loading cleaned corpora...\")\n",
    "df_train = pd.read_csv(TRAIN_PATH)\n",
    "df_eval  = pd.read_csv(EVAL_PATH)\n",
    "\n",
    "dataset_train = Dataset.from_pandas(df_train)\n",
    "dataset_eval  = Dataset.from_pandas(df_eval)\n",
    "\n",
    "print(f\"Train samples: {len(dataset_train):,}\")\n",
    "print(f\"Eval samples : {len(dataset_eval):,}\")\n",
    "\n",
    "# ===============================================================\n",
    "# LOAD MODEL + TOKENIZER\n",
    "# ===============================================================\n",
    "tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)\n",
    "model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)\n",
    "\n",
    "assert tokenizer.pad_token_id is not None, \"Tokenizer must define PAD token.\"\n",
    "\n",
    "# ===============================================================\n",
    "# TOKENIZATION\n",
    "# ===============================================================\n",
    "def preprocess(batch):\n",
    "    model_inputs = tokenizer(\n",
    "        batch[\"phonemes\"],\n",
    "        max_length=MAX_LENGTH,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    labels = tokenizer(\n",
    "        batch[\"text\"],\n",
    "        max_length=MAX_LENGTH,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "    )[\"input_ids\"]\n",
    "\n",
    "    # Replace padding token with -100 for loss masking\n",
    "    labels = [\n",
    "        [(tok if tok != tokenizer.pad_token_id else -100) for tok in seq]\n",
    "        for seq in labels\n",
    "    ]\n",
    "\n",
    "    model_inputs[\"labels\"] = labels\n",
    "    return model_inputs\n",
    "\n",
    "print(\"Tokenizing...\")\n",
    "tokenized_train = dataset_train.map(preprocess, batched=True, remove_columns=dataset_train.column_names)\n",
    "tokenized_eval  = dataset_eval.map(preprocess, batched=True, remove_columns=dataset_eval.column_names)\n",
    "\n",
    "# ===============================================================\n",
    "# METRICS\n",
    "# ===============================================================\n",
    "IGNORE_ID = -100\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    preds, labels = eval_pred\n",
    "\n",
    "    preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    # Replace masked label IDs (-100) with pad token so decoding works\n",
    "    labels = [\n",
    "        [(tok if tok != IGNORE_ID else tokenizer.pad_token_id) for tok in seq]\n",
    "        for seq in labels\n",
    "    ]\n",
    "    labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    return {\n",
    "        \"cer\"  : cer.compute(predictions=preds, references=labels),\n",
    "        \"bleu\" : bleu.compute(predictions=preds, references=labels)[\"bleu\"],\n",
    "        \"chrf\" : chrf.compute(predictions=preds, references=labels)[\"score\"]\n",
    "    }\n",
    "\n",
    "# ===============================================================\n",
    "# DATA COLLATOR\n",
    "# ===============================================================\n",
    "collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "# ===============================================================\n",
    "# TRAINING ARGUMENTS (aligned with prior notebook)\n",
    "# ===============================================================\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-4,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=8,\n",
    "\n",
    "    fp16=True,\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.1,\n",
    "    max_grad_norm=1.0,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "\n",
    "    logging_steps=100,\n",
    "    save_steps=1000,\n",
    "    eval_steps=1000,\n",
    "    predict_with_generate=True,\n",
    "\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "# ===============================================================\n",
    "# TRAINER\n",
    "# ===============================================================\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_eval,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# ===============================================================\n",
    "# TRAIN\n",
    "# ===============================================================\n",
    "print(\"Starting training...\")\n",
    "train_result = trainer.train()\n",
    "\n",
    "# Save model + tokenizer\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "print(f\"Model saved to {OUTPUT_DIR}\")\n",
    "\n",
    "# ===============================================================\n",
    "# FINAL EVALUATION\n",
    "# ===============================================================\n",
    "print(\"Running final evaluation...\")\n",
    "metrics = trainer.evaluate()\n",
    "print(metrics)\n",
    "\n",
    "# ===============================================================\n",
    "# WRITE RESULTS TO FILE\n",
    "# ===============================================================\n",
    "with open(\"training_results.txt\", \"w\") as f:\n",
    "    f.write(\"=== TRAINING & EVALUATION RESULTS ===\\n\")\n",
    "    f.write(str(metrics))\n",
    "    f.write(\"\\n\\nTraining Summary:\\n\")\n",
    "    f.write(str(train_result))\n",
    "\n",
    "print(\"Training results saved to training_results.txt\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
