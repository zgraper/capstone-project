{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6b3lBFj3scIV"
      },
      "source": [
        "# ============================================================\n",
        "#  BOOKCORPUS  ‚Üí  PHONEME/TEXT  DATASET  (to Google Drive)\n",
        "# ============================================================\n",
        "\n",
        "Zane Graper\n",
        "\n",
        "Capstone Project\n",
        "\n",
        "This notebook constructs a large, high-quality phoneme/text paired dataset from BookCorpus to support training an IPA-to-text model for child-speech ASR. The workflow addresses three persistent challenges highlighted in child-speech literature: scarcity of labeled child data, high acoustic/phonological variability, and the limits of dictionary-based correction approaches. Prior studies show that children exhibit systematic phonological substitutions, omissions, and developmental error patterns (e.g., stopping, gliding, cluster reduction), but these account for only a minority of ASR errors, meaning acoustic mismatch remains a major obstacle . Because large child corpora such as MyST and PF-STAR are limited and expensive to annotate, many pipelines rely on transfer from adult speech and synthetic augmentation . Creating a large aligned corpus of phonemes and text from clean adult data‚Äîlike BookCorpus‚Äîprovides a stable foundation for supervised training, enabling downstream models to generalize to noisier phonological patterns found in children‚Äôs speech. This notebook implements that foundation.\n",
        "\n",
        "---\n",
        "\n",
        "### Step 1: Install Prerequisites"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-EtmllLGmTCS"
      },
      "outputs": [],
      "source": [
        "# ---- 1.  Install prerequisites ----\n",
        "!pip install -q phonemizer==2.2.1\n",
        "!apt-get -qq install espeak-ng\n",
        "!pip install -q g2p-en pandas tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Mount Google Drive"
      ],
      "metadata": {
        "id": "YPA6lWGpqrA8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5o6MMrYIsIeC",
        "outputId": "bb4fd6c3-60c6-4f36-973a-cf4e89cf7800"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# ---- 2.  Mount Google Drive ----\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Imports & Paths\n",
        "\n",
        "Initializes project directories, file paths, and shared dependencies to manage the BookCorpus workflow."
      ],
      "metadata": {
        "id": "XcBOICrhqxpa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WbMfD7fpsU45"
      },
      "outputs": [],
      "source": [
        "# ---- 3. Imports & Paths ----\n",
        "import os\n",
        "import tarfile\n",
        "import urllib.request\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from phonemizer import phonemize\n",
        "base_dir = \"/content/drive/MyDrive/Capstone/Corpus\"\n",
        "os.makedirs(base_dir, exist_ok=True)\n",
        "archive_path = os.path.join(base_dir, \"bookcorpus.tar.bz2\")\n",
        "extract_dir  = os.path.join(base_dir, \"bookcorpus_raw\")\n",
        "output_csv   = os.path.join(base_dir, \"bookcorpus_phoneme_text_pairs.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4: Download Archive\n",
        "\n",
        "Fetches the 1.1 GB BookCorpus dataset directly from Hugging Face storage."
      ],
      "metadata": {
        "id": "lHCe8lfrq4H_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gaCJMoVesWgF",
        "outputId": "b8893434-e51b-430b-e4ab-4fb02fa8f367"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading BookCorpus‚Ä¶\n",
            "‚úÖ Downloaded to: /content/drive/MyDrive/Capstone/Corpus/bookcorpus.tar.bz2\n"
          ]
        }
      ],
      "source": [
        "# ---- 4.  Download archive (1.1 GB) ----\n",
        "import urllib.request\n",
        "url = \"https://storage.googleapis.com/huggingface-nlp/datasets/bookcorpus/bookcorpus.tar.bz2\"\n",
        "print(\"Downloading BookCorpus‚Ä¶\")\n",
        "urllib.request.urlretrieve(url, archive_path)\n",
        "print(\"‚úÖ Downloaded to:\", archive_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 5: Extract\n",
        "\n",
        "Unpacks the downloaded `.tar.bz2` archive into a structured directory of raw text files for processing."
      ],
      "metadata": {
        "id": "_KUvKfjkq9Id"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3f2UAm5NshkY",
        "outputId": "dccc7d09-5282-48d6-9b1b-04fd5b8efa94"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting‚Ä¶\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-4145218142.py:5: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
            "  tar.extractall(path=extract_dir)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Extracted to: /content/drive/MyDrive/Capstone/Corpus/bookcorpus_raw\n"
          ]
        }
      ],
      "source": [
        "# ---- 5.  Extract ----\n",
        "import tarfile\n",
        "print(\"Extracting‚Ä¶\")\n",
        "with tarfile.open(archive_path, \"r:bz2\") as tar:\n",
        "    tar.extractall(path=extract_dir)\n",
        "print(\"‚úÖ Extracted to:\", extract_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 6: Reservoir Sampling\n",
        "\n",
        "Randomly selects ~1M sentences across all BookCorpus files using a memory-efficient reservoir-sampling algorithm while discarding extremely short lines."
      ],
      "metadata": {
        "id": "ZizlDivarEwR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZf3HEbQ9cXy",
        "outputId": "85f62f73-cfc2-4e78-80f8-3bcb761f2eeb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2 text files.\n",
            "\n",
            "Sampling 1,000,000 sentences from BookCorpus...\n",
            "‚úÖ Sampled 1,000,000 lines from ‚âà74,004,228 total.\n",
            "\n",
            "‚úÖ Saved sample to: /content/drive/MyDrive/Capstone/Corpus/bookcorpus_sample_1m.csv\n",
            "\n",
            "Example lines:\n",
            "- `` her mother does n't know it , but anya has already tried shapeshifting .\n",
            "- i shimmied my skirt down and made a note to find my underwear before we left .\n",
            "- `` she 's not the neatest roommate in the world . ''\n",
            "- they tilted their heads , taking in my candy-stripped uniform complete with white visor and shook their heads .\n",
            "- what kind of a pervert does that ?\n"
          ]
        }
      ],
      "source": [
        "#  STEP 6 Sampling\n",
        "\n",
        "import os, random, pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ---- Paths ----\n",
        "base_dir = \"/content/drive/MyDrive/Capstone/Corpus\"\n",
        "extract_dir = os.path.join(base_dir, \"bookcorpus_raw\")\n",
        "output_csv  = os.path.join(base_dir, \"bookcorpus_sample_1m.csv\")\n",
        "\n",
        "# ---- Parameters ----\n",
        "SAMPLE_SIZE = 1_000_000     # number of sentences to keep\n",
        "MIN_WORDS   = 2            # skip ultra-short fragments\n",
        "random.seed(42)\n",
        "\n",
        "# ---- Gather all text files ----\n",
        "text_files = []\n",
        "for root, _, files in os.walk(extract_dir):\n",
        "    for f in files:\n",
        "        if f.endswith(\".txt\"):\n",
        "            text_files.append(os.path.join(root, f))\n",
        "print(f\"Found {len(text_files)} text files.\\n\")\n",
        "\n",
        "# ---- Reservoir sampling (memory-safe random sampling) ----\n",
        "sample = []\n",
        "total_lines = 0\n",
        "print(f\"Sampling {SAMPLE_SIZE:,} sentences from BookCorpus...\")\n",
        "\n",
        "for path in text_files:\n",
        "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "        for line in f:\n",
        "            total_lines += 1\n",
        "            text = line.strip()\n",
        "            if not text or len(text.split()) < MIN_WORDS:\n",
        "                continue\n",
        "\n",
        "            if len(sample) < SAMPLE_SIZE:\n",
        "                sample.append(text)\n",
        "            else:\n",
        "                j = random.randint(0, total_lines)\n",
        "                if j < SAMPLE_SIZE:\n",
        "                    sample[j] = text\n",
        "\n",
        "print(f\"‚úÖ Sampled {len(sample):,} lines from ‚âà{total_lines:,} total.\\n\")\n",
        "\n",
        "# ---- Save sample to CSV ----\n",
        "df = pd.DataFrame(sample, columns=[\"text\"])\n",
        "df.to_csv(output_csv, index=False, encoding=\"utf-8\")\n",
        "print(f\"‚úÖ Saved sample to: {output_csv}\")\n",
        "\n",
        "# ---- Quick sanity check ----\n",
        "print(\"\\nExample lines:\")\n",
        "for t in sample[:5]:\n",
        "    print(\"-\", t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "im_YPyraXhGJ",
        "outputId": "7d41141c-9c12-4a7b-f829-df2143613a9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n",
            "[nltk_data] Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]   Package cmudict is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DH AH0   K AE1 T AH0 P IH2 L ER0   W IH1 DH   AH0   SH EH1 L   ER0 AW1 N D   IH1 T   IH1 Z   K AO1 L D   AH0   P IY1   .\n"
          ]
        }
      ],
      "source": [
        "### TEST PHOMENIZER\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nltk.download('cmudict')\n",
        "nltk.download('punkt')\n",
        "\n",
        "from g2p_en import G2p\n",
        "g2p = G2p()\n",
        "print(\" \".join(g2p(\"The caterpillar with a shell around it is called a pea.\")))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 7: Clean text prior to G2P\n",
        "\n",
        "Normalizes punctuation, spacing, and length constraints to remove noisy artifacts and ensure text is well-formed before phonemization."
      ],
      "metadata": {
        "id": "3QODL91nrO-5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  STEP 7  ‚Äî  CLEAN TEXT PRIOR TO G2P\n",
        "\n",
        "import os, re, pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ---- Paths ----\n",
        "base_dir  = \"/content/drive/MyDrive/Capstone/Corpus\"\n",
        "input_csv = os.path.join(base_dir, \"bookcorpus_sample_1m.csv\")\n",
        "clean_csv = os.path.join(base_dir, \"bookcorpus_sample_1m_clean.csv\")\n",
        "\n",
        "# ---- Parameters ----\n",
        "MIN_WORDS = 5          # drop 2-word or shorter lines\n",
        "MAX_WORDS = 150        # drop abnormally long sentences\n",
        "\n",
        "# ---- Helper: normalize punctuation etc. ----\n",
        "def preclean_text(t: str) -> str:\n",
        "    t = str(t).strip()\n",
        "    t = re.sub(r\"[‚Äú‚Äù]\", '\"', t)\n",
        "    t = re.sub(r\"[‚Äò‚Äô]\", \"'\", t)\n",
        "    t = re.sub(r\"[‚Äì‚Äî]\", \"-\", t)\n",
        "    t = re.sub(r\"[\\r\\n]+\", \" \", t)\n",
        "    t = re.sub(r\"\\s+\", \" \", t)\n",
        "    return t\n",
        "\n",
        "# ---- Load and clean ----\n",
        "print(f\"Loading {input_csv}...\")\n",
        "df = pd.read_csv(input_csv)\n",
        "\n",
        "print(\"Cleaning text and filtering...\")\n",
        "cleaned_rows = []\n",
        "for line in tqdm(df[\"text\"].astype(str), total=len(df)):\n",
        "    text = preclean_text(line)\n",
        "    word_count = len(text.split())\n",
        "    if MIN_WORDS <= word_count <= MAX_WORDS:\n",
        "        cleaned_rows.append(text)\n",
        "\n",
        "# ---- Save cleaned file ----\n",
        "clean_df = pd.DataFrame(cleaned_rows, columns=[\"text\"])\n",
        "clean_df.to_csv(clean_csv, index=False, encoding=\"utf-8\")\n",
        "print(f\"‚úÖ Saved cleaned corpus with {len(clean_df):,} rows ‚Üí {clean_csv}\")\n",
        "\n",
        "# ---- Quick sanity check ----\n",
        "print(\"\\nExample cleaned lines:\")\n",
        "for t in clean_df.sample(5, random_state=42)[\"text\"]:\n",
        "    print(\"-\", t)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T-j6iJFQdwYf",
        "outputId": "b310ec0a-9bbe-4dbc-a304-628976bcdb9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading /content/drive/MyDrive/Capstone/Corpus/bookcorpus_sample_1m.csv...\n",
            "Cleaning text and filtering...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000000/1000000 [00:13<00:00, 74255.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Saved cleaned corpus with 892,719 rows ‚Üí /content/drive/MyDrive/Capstone/Corpus/bookcorpus_sample_1m_clean.csv\n",
            "\n",
            "Example cleaned lines:\n",
            "- `` which they ... love , '' sanya said .\n",
            "- he had a long braid down his back , just as dragon did and wore his vest open like the other males .\n",
            "- i follow behind the old woman in a state of half-consciousness .\n",
            "- i 've got to be heading back . ''\n",
            "- she felt the engorgement within her tremble ; felt the demon try , at least momentarily , to draw back and regroup .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 8: Phonemize BookCorpus (G2P-EN)\n",
        "\n",
        "Streams through the cleaned corpus, converts each line to ARPAbet phonemes using `g2p-en`, and writes phoneme/text pairs with resume-safe logic."
      ],
      "metadata": {
        "id": "m-VjoFo9rVqU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZRfYLytsmuD",
        "outputId": "62fdec2e-c2ed-4b4c-ebf5-b09a6b26d2a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting new phonemization output file.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Phonemizing (g2p-en): 892719 lines [29:58, 496.27 lines/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Streaming phonemization complete (g2p-en).\n",
            "Results saved at: /content/drive/MyDrive/Capstone/Corpus/bookcorpus_sample_1m_phonemes.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "#  STEP 8 - PHONEMIZE BOOKCORPUS CSV  ‚Üí  PHONEME/TEXT PAIRS\n",
        "\n",
        "import csv, os, pandas as pd, tqdm\n",
        "from tqdm import tqdm\n",
        "from g2p_en import G2p\n",
        "g2p = G2p()\n",
        "\n",
        "# ---- Paths ----\n",
        "base_dir   = \"/content/drive/MyDrive/Capstone/Corpus\"\n",
        "input_csv  = os.path.join(base_dir, \"bookcorpus_sample_1m_clean.csv\")\n",
        "output_csv = os.path.join(base_dir, \"bookcorpus_sample_1m_phonemes.csv\")\n",
        "\n",
        "# ---- Resume support ----\n",
        "processed_lines = 0\n",
        "if os.path.exists(output_csv):\n",
        "    # count existing lines (minus header)\n",
        "    with open(output_csv, \"r\", encoding=\"utf-8\") as f:\n",
        "        processed_lines = sum(1 for _ in f) - 1\n",
        "    print(f\"Resuming after {processed_lines:,} processed lines.\")\n",
        "else:\n",
        "    # create new output with header\n",
        "    with open(output_csv, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([\"phonemes\", \"text\"])\n",
        "    print(\"Starting new phonemization output file.\")\n",
        "\n",
        "# ---- Streaming loop ----\n",
        "from g2p_en import G2p\n",
        "g2p = G2p()\n",
        "\n",
        "with open(input_csv, \"r\", encoding=\"utf-8\") as infile, \\\n",
        "     open(output_csv, \"a\", encoding=\"utf-8\", newline=\"\") as outfile:\n",
        "\n",
        "    reader = csv.DictReader(infile)\n",
        "    writer = csv.writer(outfile)\n",
        "\n",
        "    for i, row in enumerate(tqdm(reader, desc=\"Phonemizing (g2p-en)\", unit=\" lines\")):\n",
        "        if i < processed_lines:\n",
        "            continue\n",
        "        text = row[\"text\"].strip()\n",
        "        if not text:\n",
        "            continue\n",
        "        try:\n",
        "            phon = \" \".join(g2p(text))\n",
        "            if phon.strip():\n",
        "                writer.writerow([phon, text])\n",
        "                outfile.flush()\n",
        "        except Exception as e:\n",
        "            print(f\"[skip] {e}\")\n",
        "            continue\n",
        "\n",
        "print(\"‚úÖ Streaming phonemization complete (g2p-en).\")\n",
        "print(f\"Results saved at: {output_csv}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 9: Celan after G2P\n",
        "\n",
        "Removes stress markers/digits, normalizes both phonemes and text, filters extreme lengths, and drops duplicates to improve model-readiness."
      ],
      "metadata": {
        "id": "k8myQbMbreWh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dWnEbzyVVpw5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32d2cee5-8d3e-4eb5-fbc9-20b79809946d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading /content/drive/MyDrive/Capstone/Corpus/bookcorpus_sample_1m_phonemes.csv...\n",
            "Cleaning phonemes and text...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "phonemes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 892719/892719 [00:23<00:00, 38675.70it/s]\n",
            "text: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 892719/892719 [00:07<00:00, 116126.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Cleaned dataset saved with 788,671 rows ‚Üí /content/drive/MyDrive/Capstone/Corpus/bookcorpus_1m_final.csv\n",
            "\n",
            "Sample rows:\n",
            "HH ER M AH DH ER D AH Z EH N T AY N OW IH T B AH T EH N Y AH HH AE Z AO L R EH D IY T R AY D SH EY P SH AH F IH SH T  ||  her mother does n't know it but anya has already tried shapeshifting\n",
            "AY SH IH M IY D M AY S K ER T D AW N AH N D M EY D AH N OW T T UW F AY N D M AY AH N D ER W EH R B IH F AO R W IY L EH F T  ||  i shimmied my skirt down and made a note to find my underwear before we left\n",
            "SH IY EH S N AA T DH AH N IY T AH S T R UW M EY T IH N DH AH W ER L D  ||  she 's not the neatest roommate in the world ''\n"
          ]
        }
      ],
      "source": [
        "#  STEP 9 ‚Äî CLEAN AFTER G2P (PHONEME/TEXT NORMALIZATION)\n",
        "\n",
        "import os, re, pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ---- Paths ----\n",
        "base_dir   = \"/content/drive/MyDrive/Capstone/Corpus\"\n",
        "input_csv  = os.path.join(base_dir, \"bookcorpus_sample_1m_phonemes.csv\")\n",
        "output_csv = os.path.join(base_dir, \"bookcorpus_1m_final.csv\")\n",
        "\n",
        "# ---- Parameters ----\n",
        "MIN_WORDS = 5\n",
        "MAX_WORDS = 150\n",
        "\n",
        "# ---- Helpers ----\n",
        "def clean_phonemes(p: str) -> str:\n",
        "    \"\"\"Remove stress digits, punctuation, and normalize spacing.\"\"\"\n",
        "    p = str(p)\n",
        "    p = re.sub(r\"\\d\", \"\", p)          # drop stress markers\n",
        "    p = re.sub(r\"[^A-Z\\s]\", \" \", p)   # keep uppercase letters and spaces only\n",
        "    p = re.sub(r\"\\s+\", \" \", p).strip()\n",
        "    return p\n",
        "\n",
        "def clean_text(t: str) -> str:\n",
        "    \"\"\"Normalize text to lowercase and remove odd punctuation.\"\"\"\n",
        "    t = str(t).lower()\n",
        "    t = re.sub(r\"[^a-z0-9\\s']\", \" \", t)   # keep letters, numbers, apostrophes\n",
        "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
        "    return t\n",
        "\n",
        "# ---- Load ----\n",
        "print(f\"Loading {input_csv}...\")\n",
        "df = pd.read_csv(input_csv)\n",
        "\n",
        "# ---- Drop rows missing phonemes or text ----\n",
        "df = df.dropna(subset=[\"phonemes\", \"text\"])\n",
        "\n",
        "# ---- Clean ----\n",
        "print(\"Cleaning phonemes and text...\")\n",
        "df[\"phonemes\"] = [clean_phonemes(p) for p in tqdm(df[\"phonemes\"], desc=\"phonemes\")]\n",
        "df[\"text\"]     = [clean_text(t)     for t in tqdm(df[\"text\"], desc=\"text\")]\n",
        "\n",
        "# ---- Filter extreme lengths ----\n",
        "df[\"phon_len\"] = df[\"phonemes\"].apply(lambda x: len(x.split()))\n",
        "df[\"text_len\"] = df[\"text\"].apply(lambda x: len(x.split()))\n",
        "df = df[df[\"phon_len\"].between(MIN_WORDS, MAX_WORDS)]\n",
        "df = df[df[\"text_len\"].between(MIN_WORDS, MAX_WORDS)]\n",
        "\n",
        "# ---- Drop duplicates and reset index ----\n",
        "df = df.drop_duplicates(subset=[\"phonemes\", \"text\"]).reset_index(drop=True)\n",
        "\n",
        "# ---- Save ----\n",
        "df[[\"phonemes\", \"text\"]].to_csv(output_csv, index=False, encoding=\"utf-8\")\n",
        "print(f\"‚úÖ Cleaned dataset saved with {len(df):,} rows ‚Üí {output_csv}\")\n",
        "\n",
        "# ---- Quick sanity check ----\n",
        "print(\"\\nSample rows:\")\n",
        "for i in range(3):\n",
        "    print(f\"{df.iloc[i]['phonemes']}  ||  {df.iloc[i]['text']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 10: Sample Check (50 pairs)\n",
        "\n",
        "Randomly displays dozens of phoneme/text pairs to manually verify correctness and phonemic consistency."
      ],
      "metadata": {
        "id": "ap3_H6m8rn9C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iGYNLtFk43RV",
        "outputId": "e801353b-445f-4f96-e3cd-95ec4b59e17f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Showing 50 random phoneme-text pairs from 788,671 total rows:\n",
            "\n",
            "üü¢ Sample 1\n",
            "Text:    on the refrigerator there was a small piece of paper with the number of her parents ' hotel in paris\n",
            "Phoneme: AA N DH AH R AH F R IH JH ER EY T ER DH EH R W AA Z AH S M AO L P IY S AH V P EY P ER W IH DH DH AH N AH M B ER AH V HH ER P EH R AH N T S HH OW T EH L IH N P EH R IH S\n",
            "\n",
            "üü¢ Sample 2\n",
            "Text:    i breathed out relieved and yet not quite able to feel at ease\n",
            "Phoneme: AY B R IY DH D AW T R IH L IY V D AH N D Y EH T N AA T K W AY T EY B AH L T UW F IY L AE T IY Z\n",
            "\n",
            "üü¢ Sample 3\n",
            "Text:    no identification on either man but one looks to be malaysian or indonesian and the other could be viet or laotian or possibly cambodian\n",
            "Phoneme: N OW AY D EH N T AH F AH K EY SH AH N AA N IY DH ER M AE N B AH T W AH N L UH K S T UW B IY M AH L EY ZH AH N AO R IH N D OW N IY ZH AH N AH N D DH AH AH DH ER K UH D B IY V IY EH T AO R L EY OW SH AH N AO R P AA S AH B L IY K AE M B OW D IY AH N\n",
            "\n",
            "üü¢ Sample 4\n",
            "Text:    nothing '' he said looking ahead absently\n",
            "Phoneme: N AH TH IH NG HH IY S EH D L UH K IH NG AH HH EH D AE B S T AH N T L IY\n",
            "\n",
            "üü¢ Sample 5\n",
            "Text:    that 's probably because i feel like i 'm going to puke\n",
            "Phoneme: DH AE T EH S P R AA B AH B L IY B IH K AO Z AY F IY L L AY K AY EH M G OW IH NG T UW P Y UW K\n",
            "\n",
            "üü¢ Sample 6\n",
            "Text:    little did the group know but boone and delilah had never seen each other totally naked let alone had sex\n",
            "Phoneme: L IH T AH L D IH D DH AH G R UW P N OW B AH T B UW N AH N D D AH L AY L AH HH AE D N EH V ER S IY N IY CH AH DH ER T OW T AH L IY N EY K AH D L EH T AH L OW N HH AE D S EH K S\n",
            "\n",
            "üü¢ Sample 7\n",
            "Text:    it would just make things worse\n",
            "Phoneme: IH T W UH D JH AH S T M EY K TH IH NG Z W ER S\n",
            "\n",
            "üü¢ Sample 8\n",
            "Text:    you know the phrase whoever smelt it dealt it ''\n",
            "Phoneme: Y UW N OW DH AH F R EY Z HH UW EH V ER S M EH L T IH T D EH L T IH T\n",
            "\n",
            "üü¢ Sample 9\n",
            "Text:    he leaned a little closer and lowered his voice\n",
            "Phoneme: HH IY L IY N D AH L IH T AH L K L OW S ER AH N D L OW ER D HH IH Z V OY S\n",
            "\n",
            "üü¢ Sample 10\n",
            "Text:    but it is love nonetheless\n",
            "Phoneme: B AH T IH T IH Z L AH V N AH N DH AH L EH S\n",
            "\n",
            "üü¢ Sample 11\n",
            "Text:    alayna 's certainly not scared\n",
            "Phoneme: AH L EY N AH EH S S ER T AH N L IY N AA T S K EH R D\n",
            "\n",
            "üü¢ Sample 12\n",
            "Text:    get a bid from each of them then we 'll go from there\n",
            "Phoneme: G EH T AH B IH D F R AH M IY CH AH V DH EH M DH EH N W IY L EH L G OW F R AH M DH EH R\n",
            "\n",
            "üü¢ Sample 13\n",
            "Text:    because i 'm pretty sure it 's not 'a hee a hee a hee '\n",
            "Phoneme: B IH K AO Z AY EH M P R IH T IY SH UH R IH T EH S N AA T EY HH IY EY HH IY EY HH IY\n",
            "\n",
            "üü¢ Sample 14\n",
            "Text:    i know that i 'm the one who should be apologizing but i take a bite of pizza instead\n",
            "Phoneme: AY N OW DH AE T AY EH M DH AH W AH N HH UW SH UH D B IY AH P AA L AH JH AY Z IH NG B AH T AY T EY K AH B AY T AH V P IY T S AH IH N S T EH D\n",
            "\n",
            "üü¢ Sample 15\n",
            "Text:    the storm was still in full force when they got back\n",
            "Phoneme: DH AH S T AO R M W AA Z S T IH L IH N F UH L F AO R S W EH N DH EY G AA T B AE K\n",
            "\n",
            "üü¢ Sample 16\n",
            "Text:    max was sitting in his office when she walked in\n",
            "Phoneme: M AE K S W AA Z S IH T IH NG IH N HH IH Z AO F AH S W EH N SH IY W AO K T IH N\n",
            "\n",
            "üü¢ Sample 17\n",
            "Text:    in the kitchen the cake was cooling on a platter\n",
            "Phoneme: IH N DH AH K IH CH AH N DH AH K EY K W AA Z K UW L IH NG AA N AH P L AE T ER\n",
            "\n",
            "üü¢ Sample 18\n",
            "Text:    i 'd gotten them up because i needed to give them instructions before i left and they started their days\n",
            "Phoneme: AY D IY G AA T AH N DH EH M AH P B IH K AO Z AY N IY D AH D T UW G IH V DH EH M IH N S T R AH K SH AH N Z B IH F AO R AY L EH F T AH N D DH EY S T AA R T AH D DH EH R D EY Z\n",
            "\n",
            "üü¢ Sample 19\n",
            "Text:    bax fought to hold the line\n",
            "Phoneme: B AE K S F AO T T UW HH OW L D DH AH L AY N\n",
            "\n",
            "üü¢ Sample 20\n",
            "Text:    my day kept getting shittier and i knew the best thing for me was to go home\n",
            "Phoneme: M AY D EY K EH P T G EH T IH NG SH IH T IY ER AH N D AY N UW DH AH B EH S T TH IH NG F AO R M IY W AA Z T UW G OW HH OW M\n",
            "\n",
            "üü¢ Sample 21\n",
            "Text:    she gave a flip of both wings and twirled in a spiral\n",
            "Phoneme: SH IY G EY V AH F L IH P AH V B OW TH W IH NG Z AH N D T W ER L D IH N AH S P AY R AH L\n",
            "\n",
            "üü¢ Sample 22\n",
            "Text:    it is a yellow beast with a long neck\n",
            "Phoneme: IH T IH Z AH Y EH L OW B IY S T W IH DH AH L AO NG N EH K\n",
            "\n",
            "üü¢ Sample 23\n",
            "Text:    she prefers the old historic hotels in the city centers even if the beds are bumpy and the plumbing drips rust stains onto the curving porcelain sinks\n",
            "Phoneme: SH IY P R AH F ER Z DH AH OW L D HH IH S T AO R IH K HH OW T EH L Z IH N DH AH S IH T IY S EH N T ER Z IY V IH N IH F DH AH B EH D Z AA R B AH M P IY AH N D DH AH P L AH M IH NG D R IH P S R AH S T S T EY N Z AA N T UW DH AH K ER V IH NG P AO R S AH L AH N S IH NG K S\n",
            "\n",
            "üü¢ Sample 24\n",
            "Text:    it was never a case of if he would do it but rather a question of whether or not you could handle a particular consequence of denying him\n",
            "Phoneme: IH T W AA Z N EH V ER AH K EY S AH V IH F HH IY W UH D D UW IH T B AH T R AE DH ER AH K W EH S CH AH N AH V W EH DH ER AO R N AA T Y UW K UH D HH AE N D AH L AH P ER T IH K Y AH L ER K AA N S AH K W AH N S AH V D IH N AY IH NG HH IH M\n",
            "\n",
            "üü¢ Sample 25\n",
            "Text:    the young man opened his arms wide\n",
            "Phoneme: DH AH Y AH NG M AE N OW P AH N D HH IH Z AA R M Z W AY D\n",
            "\n",
            "üü¢ Sample 26\n",
            "Text:    caltoan glanced at the clock in the dash\n",
            "Phoneme: K AE L T OW N G L AE N S T AE T DH AH K L AA K IH N DH AH D AE SH\n",
            "\n",
            "üü¢ Sample 27\n",
            "Text:    from then on margaret joins in whenever the five eat together\n",
            "Phoneme: F R AH M DH EH N AA N M AA R G ER IH T JH OY N Z IH N W EH N EH V ER DH AH F AY V IY T T AH G EH DH ER\n",
            "\n",
            "üü¢ Sample 28\n",
            "Text:    how would crazed zealot eibhear handle this ''\n",
            "Phoneme: HH AW W UH D K R EY Z AH T AO L D AY B HH ER HH AE N D AH L DH IH S\n",
            "\n",
            "üü¢ Sample 29\n",
            "Text:    i would like to know which one of you idiots locked the door after i left earlier i angrily demanded\n",
            "Phoneme: AY W UH D L AY K T UW N OW W IH CH W AH N AH V Y UW IH D IY AH T S L AA K T DH AH D AO R AE F T ER AY L EH F T ER L IY ER AY AE NG G R AH L IY D IH M AE N D AH D\n",
            "\n",
            "üü¢ Sample 30\n",
            "Text:    you do n't like it stay out\n",
            "Phoneme: Y UW D UW EH N T AY L AY K IH T S T EY AW T\n",
            "\n",
            "üü¢ Sample 31\n",
            "Text:    martinez burn the jets ''\n",
            "Phoneme: M AA R T IY N EH Z B ER N DH AH JH EH T S\n",
            "\n",
            "üü¢ Sample 32\n",
            "Text:    when he said nothing bax finally said well\n",
            "Phoneme: W EH N HH IY S EH D N AH TH IH NG B AE K S F AY N AH L IY S EH D W EH L\n",
            "\n",
            "üü¢ Sample 33\n",
            "Text:    three pairs of handcuffs had been fastened the other side\n",
            "Phoneme: TH R IY P EH R Z AH V HH AE N D K AH F S HH AE D B IH N F AE S AH N D DH AH AH DH ER S AY D\n",
            "\n",
            "üü¢ Sample 34\n",
            "Text:    violet reached over put her young smooth hand on top of warren 's brown speckled one and stopped his rocking\n",
            "Phoneme: V AY AH L IH T R IY CH T OW V ER P UH T HH ER Y AH NG S M UW DH HH AE N D AA N T AA P AH V W AO R AH N EH S B R AW N S P EH K AH L D W AH N AH N D S T AA P T HH IH Z R AA K IH NG\n",
            "\n",
            "üü¢ Sample 35\n",
            "Text:    a metal staircase on pulleys so it could be lowered over the wall for the combatants to enter was sticking straight up at the front of the pit\n",
            "Phoneme: AH M EH T AH L S T EH R K EY S AA N P UH L IY Z S OW IH T K UH D B IY L OW ER D OW V ER DH AH W AO L F AO R DH AH K AH M B AE T AH N T S T UW EH N T ER W AA Z S T IH K IH NG S T R EY T AH P AE T DH AH F R AH N T AH V DH AH P IH T\n",
            "\n",
            "üü¢ Sample 36\n",
            "Text:    half the contingent saw him get swarmed by chalklings\n",
            "Phoneme: HH AE F DH AH K AH N T IH N JH AH N T S AO HH IH M G EH T S W AO R M D B AY CH AE L K AH NG Z\n",
            "\n",
            "üü¢ Sample 37\n",
            "Text:    she walked my bag down the hall calling over her shoulder you can help me with dinner ''\n",
            "Phoneme: SH IY W AO K T M AY B AE G D AW N DH AH HH AO L K AO L IH NG OW V ER HH ER SH OW L D ER Y UW K AE N HH EH L P M IY W IH DH D IH N ER\n",
            "\n",
            "üü¢ Sample 38\n",
            "Text:    that 's what we do take care of one another ''\n",
            "Phoneme: DH AE T EH S W AH T W IY D UW T EY K K EH R AH V W AH N AH N AH DH ER\n",
            "\n",
            "üü¢ Sample 39\n",
            "Text:    your instincts are strong come up i have something to show you\n",
            "Phoneme: Y AO R IH N S T IH NG K T S AA R S T R AO NG K AH M AH P AY HH AE V S AH M TH IH NG T UW SH OW Y UW\n",
            "\n",
            "üü¢ Sample 40\n",
            "Text:    with a sigh she closed her eyes and waited\n",
            "Phoneme: W IH DH AH S AY SH IY K L OW Z D HH ER AY Z AH N D W EY T AH D\n",
            "\n",
            "üü¢ Sample 41\n",
            "Text:    we stayed up late talking all four of us\n",
            "Phoneme: W IY S T EY D AH P L EY T T AO K IH NG AO L F AO R AH V AH S\n",
            "\n",
            "üü¢ Sample 42\n",
            "Text:    we 'll find them and persuade them it 's a bad idea to mess with us\n",
            "Phoneme: W IY L EH L F AY N D DH EH M AH N D P ER S W EY D DH EH M IH T EH S AH B AE D AY D IY AH T UW M EH S W IH DH AH S\n",
            "\n",
            "üü¢ Sample 43\n",
            "Text:    brick looked at her and shrugged with his head further along the path the move quick and it was obvious he wanted her to hurry up\n",
            "Phoneme: B R IH K L UH K T AE T HH ER AH N D SH R AH G D W IH DH HH IH Z HH EH D F ER DH ER AH L AO NG DH AH P AE TH DH AH M UW V K W IH K AH N D IH T W AA Z AA B V IY AH S HH IY W AA N T AH D HH ER T UW HH ER IY AH P\n",
            "\n",
            "üü¢ Sample 44\n",
            "Text:    elaina nodded the casting creation and everything else\n",
            "Phoneme: IH L EY N AH N AA D AH D DH AH K AE S T IH NG K R IY EY SH AH N AH N D EH V R IY TH IH NG EH L S\n",
            "\n",
            "üü¢ Sample 45\n",
            "Text:    pilar fixed each man in turn with an intense stare that bristled with quiet menace\n",
            "Phoneme: P AY L ER F IH K S T IY CH M AE N IH N T ER N W IH DH AE N IH N T EH N S S T EH R DH AE T B R IH S AH L D W IH DH K W AY AH T M EH N AH S\n",
            "\n",
            "üü¢ Sample 46\n",
            "Text:    he could ruffle the smoothest of feathers and his look of tolerant amusement caused her to amend her statement with uh sir\n",
            "Phoneme: HH IY K UH D R AH F AH L DH AH S M UW DH AH S T AH V F EH DH ER Z AH N D HH IH Z L UH K AH V T AA L ER AH N T AH M Y UW Z M AH N T K AA Z D HH ER T UW AH M EH N D HH ER S T EY T M AH N T W IH DH AH S ER\n",
            "\n",
            "üü¢ Sample 47\n",
            "Text:    the pizza had cooled a bit and i was able to eat the first slice i took\n",
            "Phoneme: DH AH P IY T S AH HH AE D K UW L D AH B IH T AH N D AY W AA Z EY B AH L T UW IY T DH AH F ER S T S L AY S AY T UH K\n",
            "\n",
            "üü¢ Sample 48\n",
            "Text:    what if they called the police\n",
            "Phoneme: W AH T IH F DH EY K AO L D DH AH P AH L IY S\n",
            "\n",
            "üü¢ Sample 49\n",
            "Text:    we use it as a card table when we play poker ''\n",
            "Phoneme: W IY Y UW Z IH T AE Z AH K AA R D T EY B AH L W EH N W IY P L EY P OW K ER\n",
            "\n",
            "üü¢ Sample 50\n",
            "Text:    in that case kaye would have shown up at your house before now to conduct an intervention ''\n",
            "Phoneme: IH N DH AE T K EY S K EY W UH D HH AE V SH OW N AH P AE T Y AO R HH AW S B IH F AO R N AW T UW K AA N D AH K T AE N IH N T ER V EH N CH AH N\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#  10. SAMPLE CHECK: View 50 random phoneme/text pairs\n",
        "\n",
        "import os, pandas as pd\n",
        "from random import sample\n",
        "\n",
        "# ---- Paths ----\n",
        "base_dir   = \"/content/drive/MyDrive/Capstone/Corpus\"\n",
        "phoneme_csv = os.path.join(base_dir, \"bookcorpus_1m_final.csv\")\n",
        "\n",
        "# ---- Load dataset (only once) ----\n",
        "df = pd.read_csv(phoneme_csv)\n",
        "\n",
        "# Guard against small files\n",
        "n = len(df)\n",
        "if n == 0:\n",
        "    raise ValueError(\"The phoneme CSV appears empty.\")\n",
        "num_samples = min(50, n)\n",
        "\n",
        "# ---- Randomly sample 50 lines across the file ----\n",
        "indices = sample(range(n), num_samples)\n",
        "subset = df.iloc[indices]\n",
        "\n",
        "# ---- Display neatly in Colab ----\n",
        "print(f\"Showing {num_samples} random phoneme-text pairs from {n:,} total rows:\\n\")\n",
        "for i, row in enumerate(subset.itertuples(index=False), 1):\n",
        "    print(f\"üü¢ Sample {i}\")\n",
        "    print(f\"Text:    {row.text}\")\n",
        "    print(f\"Phoneme: {row.phonemes}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook produces a large, clean, and phonemically aligned dataset suitable for training an IPA-to-text or phoneme-to-text model. By standardizing punctuation, normalizing phonemes, and applying strict length filtering, the workflow removes many sources of noise that can destabilize sequence-to-sequence models. The resulting dataset supports robust generalization‚Äîcritical given that child-speech ASR suffers from high acoustic and phonological variability and benefits strongly from abundant adult-speech pretraining before child-specific fine-tuning, as documented across Whisper, wav2vec2, and Conformer studies . This corpus therefore acts as the ‚Äúperfect‚Äù phoneme/text supervision necessary to stabilize downstream training before merging with child-speech data. The pipeline is modular, transparent, and reproducible, making it appropriate for inclusion in a research-grade repository.\n",
        "\n",
        "**Takeaway**: This notebook builds the foundational dataset that enables your IPA-to-text model to learn stable phoneme-text mappings before adapting to the complexities of children‚Äôs speech."
      ],
      "metadata": {
        "id": "HXt9tz6Rru7v"
      }
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}